"""
AS é¡¹ç›®çˆ¬è™« - arXiv Spider
åŠŸèƒ½: è´Ÿè´£ä» arXiv ç½‘ç«™æŠ“å–æœ€æ–°çš„è®ºæ–‡ ID å’Œåˆ†ç±»ä¿¡æ¯ã€‚
é€»è¾‘: 
1. è®¿é—® arXiv åˆ—è¡¨é¡µ (å¦‚ https://arxiv.org/list/cs.LG/new)ã€‚
2. è§£æ HTML æå–è®ºæ–‡ ID å’Œåˆ†ç±»ã€‚
3. ç”Ÿæˆ PaperItem ä¼ é€’ç»™ Pipeline è¿›è¡Œåç»­å¤„ç† (API è·å–è¯¦æƒ…)ã€‚
"""
import scrapy
import re
import os
from crawler.items import PaperItem
from dotenv import load_dotenv

load_dotenv()

class ArxivSpider(scrapy.Spider):
    name = "arxiv"
    allowed_domains = ["arxiv.org"]
    
    custom_settings = {
        "LOG_LEVEL": "INFO",
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ğŸ¤« é™éŸ³ httpx å’Œ scrapy çš„å†—ä½™æ—¥å¿—ï¼Œåªä¿ç•™ WARNING åŠä»¥ä¸Š
        import logging
        logging.getLogger("httpx").setLevel(logging.WARNING)
        logging.getLogger("scrapy").setLevel(logging.WARNING)
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ categories å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
        # categories å‚æ•°å¯èƒ½æ˜¯é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
        if hasattr(self, 'categories') and self.categories:
            categories_str = self.categories
        else:
            categories_str = os.getenv("CATEGORIES")

        if not categories_str:
            self.target_categories = set()
        else:
            self.target_categories = set(map(str.strip, categories_str.split(",")))
        
        self.start_urls = [
            f"https://arxiv.org/list/{cat}/new" for cat in self.target_categories
        ]
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            "total_found": 0,
            "yielded": 0,
            "skipped_category": 0,
            "categories_found": set()
        }

    def parse(self, response):
        self.logger.info(f"æ­£åœ¨è§£æé¡µé¢: {response.url}")
        
        # è·å–å½“å‰é¡µé¢çš„åˆ†ç±»
        current_category = response.url.split("/list/")[-1].split("/")[0]
        self.stats["categories_found"].add(current_category)
        
        # æå–é”šç‚¹
        anchors = []
        for li in response.css("div[id=dlpage] ul li"):
            href = li.css("a::attr(href)").get()
            if href and "item" in href:
                anchors.append(int(href.split("item")[-1]))
        
        self.logger.debug(f"æ‰¾åˆ° {len(anchors)} ä¸ªé”šç‚¹")
        
        dt_elements = response.xpath('//dl[@id="articles"]/dt')
        dd_elements = response.xpath('//dl[@id="articles"]/dd')
        
        items_to_yield = []

        for dt, dd in zip(dt_elements, dd_elements):
            self.stats["total_found"] += 1
            
            # æå–ArXiv ID
            paper_anchor = dt.xpath('./a[@name]/@name').get()
            if paper_anchor and "item" in paper_anchor:
                paper_id_num = int(paper_anchor.split("item")[-1])
                if anchors and paper_id_num >= anchors[-1]:
                    continue

            arxiv_id_text = dt.xpath('.//a[@title="Abstract"]/text()').get()
            if not arxiv_id_text:
                continue
            
            arxiv_id = arxiv_id_text.replace("arXiv:", "").strip()
            self.logger.debug(f"å‘ç°è®ºæ–‡: {arxiv_id}")
            
            # æå–æ‰€æœ‰åˆ†ç±» (Tags)
            # ç»“æ„: <div class="list-subjects">
            # <span class="primary-subject">Primary (cs.XX)</span>; Secondary (cs.YY)
            # </div>
            subjects_div = dd.xpath('.//div[contains(@class, "list-subjects")]')
            
            # æå–ä¸»ç±»åˆ« (Primary Subject)
            primary_subject_text = subjects_div.xpath('.//span[@class="primary-subject"]/text()').get()
            primary_category = None
            
            if primary_subject_text:
                # ä» "Computer Vision and Pattern Recognition (cs.CV)" æå– "cs.CV"
                match = re.search(r'\(([^\)]+)\)', primary_subject_text)
                if match:
                    primary_category = match.group(1)
            
            # æå–æ‰€æœ‰åˆ†ç±»æ ‡ç­¾ï¼ˆåŒ…æ‹¬ä¸»ç±»åˆ«å’Œæ¬¡ç±»åˆ«ï¼‰
            subjects_text = subjects_div.get()
            all_tags = []
            if subjects_text:
                # æå–æ‰€æœ‰æ‹¬å·å†…çš„åˆ†ç±»ä»£ç ï¼Œå¦‚ (cs.CV), (math.OT), (eess.IV)
                found_tags = re.findall(r'\(([\w\.-]+)\)', subjects_text)
                # ä½¿ç”¨ dict.fromkeys ä¿ç•™é¡ºåºå»é‡
                all_tags = list(dict.fromkeys(found_tags))
            
            # ç¡®ä¿ä¸»ç±»åˆ«åœ¨ tags åˆ—è¡¨çš„ç¬¬ä¸€ä½
            if primary_category and primary_category not in all_tags:
                all_tags.insert(0, primary_category)
            elif primary_category and primary_category in all_tags:
                # å°†ä¸»ç±»åˆ«ç§»åˆ°ç¬¬ä¸€ä½
                all_tags.remove(primary_category)
                all_tags.insert(0, primary_category)

            # [UPDATED] è¿‡æ»¤ï¼šåªè¦ä»»æ„ä¸€ä¸ªåˆ†ç±»åœ¨ç›®æ ‡ç±»åˆ«ä¸­å°±ä¿ç•™
            # æ£€æŸ¥ all_tags å’Œ target_categories æ˜¯å¦æœ‰äº¤é›†
            if not any(tag in self.target_categories for tag in all_tags):
                self.logger.debug(f"è·³è¿‡ {arxiv_id} - åˆ†ç±»ä¸åŒ¹é… {all_tags}")
                self.stats["skipped_category"] += 1
                continue

            # æ„å»º Item
            item = PaperItem()
            item["id"] = arxiv_id
            item["category"] = all_tags 
            item["title"] = ""
            item["authors"] = []
            item["published_date"] = ""
            item["links"] = {}
            item["comment"] = ""
            
            items_to_yield.append(item)
            
        # å¼€å§‹å†™å…¥æ•°æ®åº“ (å¯è§†åŒ–è¿›åº¦)
        from tqdm import tqdm
        import sys
        
        if items_to_yield:
            pbar = tqdm(
                total=len(items_to_yield), 
                desc=f"Saving {current_category} to DB", 
                unit="paper",
                file=sys.stdout,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            )
            
            for item in items_to_yield:
                self.stats["yielded"] += 1
                yield item
                pbar.update(1)
                
            pbar.close()
        else:
            self.logger.info(f"No papers found for {current_category} matching criteria.")

    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶è¾“å‡ºæ€»ç»“"""
        print("\n" + "="*50)
        print("ğŸ” ArXiv çˆ¬è™«æ‰§è¡Œæ€»ç»“")
        print("="*50)
        print(f"ğŸ“… ç›®æ ‡åˆ†ç±»: {', '.join(self.target_categories)}")
        print(f"ğŸ“‚ å®é™…æ‰«æåˆ†ç±»: {', '.join(self.stats['categories_found'])}")
        print(f"ğŸ“„ æ€»å…±å‘ç°è®ºæ–‡: {self.stats['total_found']}")
        print(f"âœ… æ•è·å¹¶æäº¤å¤„ç†: {self.stats['yielded']}")
        print(f"ğŸš« å› åˆ†ç±»ä¸ç¬¦è·³è¿‡: {self.stats['skipped_category']}")
        
        # å°è¯•è·å– Pipeline çš„ç»Ÿè®¡ä¿¡æ¯ (å¦‚æœ Pipeline æ›´æ–°äº† crawler.stats)
        inserted = self.crawler.stats.get_value('papers/inserted', 0)
        duplicates = self.crawler.stats.get_value('papers/duplicates', 0)
        failed = self.crawler.stats.get_value('papers/failed', 0)
        
        if inserted or duplicates or failed:
            print("-" * 30)
            print("ğŸ’¾ æ•°æ®åº“å¤„ç†ç»“æœ:")
            print(f"   ğŸ†• æ–°å¢å…¥åº“: {inserted}")
            print(f"   â™»ï¸ é‡å¤/å·²å­˜åœ¨: {duplicates}")
            print(f"   âŒ å¤„ç†å¤±è´¥: {failed}")
        
        print("="*50 + "\n")
