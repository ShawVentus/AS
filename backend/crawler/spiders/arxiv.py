"""
AS é¡¹ç›®çˆ¬è™« - arXiv Spider
åŠŸèƒ½: è´Ÿè´£ä» arXiv ç½‘ç«™æŠ“å–æœ€æ–°çš„è®ºæ–‡ ID å’Œåˆ†ç±»ä¿¡æ¯ã€‚
é€»è¾‘: 
1. è®¿é—® arXiv åˆ—è¡¨é¡µ (å¦‚ https://arxiv.org/list/cs.LG/new)ã€‚
2. è§£æ HTML æå–è®ºæ–‡ ID å’Œåˆ†ç±»ã€‚
3. ç”Ÿæˆ PaperItem ä¼ é€’ç»™ Pipeline è¿›è¡Œåç»­å¤„ç† (API è·å–è¯¦æƒ…)ã€‚
"""
import scrapy
import re
import os
from crawler.items import PaperItem
from dotenv import load_dotenv
from scrapy.exceptions import CloseSpider
from datetime import datetime
import sys

# ç¡®ä¿ backend æ ¹ç›®å½•åœ¨ sys.path ä¸­ï¼Œä»¥ä¾¿å¯¼å…¥ app æ¨¡å—
# Ensure backend root is in sys.path to import app modules
current_file = os.path.abspath(__file__)
crawler_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_file)))
if crawler_dir not in sys.path:
    sys.path.append(crawler_dir)

from app.utils.email_sender import email_sender
from app.core.database import get_db

load_dotenv()

class ArxivSpider(scrapy.Spider):
    name = "arxiv"
    allowed_domains = ["arxiv.org"]
    
    custom_settings = {
        "LOG_LEVEL": "INFO",
        # [NEW] é‡è¯•é…ç½®
        "RETRY_ENABLED": True,  # å¯ç”¨é‡è¯•ä¸­é—´ä»¶
        "RETRY_TIMES": 3,  # é‡è¯•æ¬¡æ•°
        "RETRY_HTTP_CODES": [500, 502, 503, 504, 522, 524, 408, 429],  # éœ€è¦é‡è¯•çš„HTTPçŠ¶æ€ç 
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ğŸ¤« é™éŸ³ httpx å’Œ scrapy çš„å†—ä½™æ—¥å¿—ï¼Œåªä¿ç•™ WARNING åŠä»¥ä¸Š
        import logging
        logging.getLogger("httpx").setLevel(logging.WARNING)
        logging.getLogger("scrapy").setLevel(logging.WARNING)
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ categories å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
        # categories å‚æ•°å¯èƒ½æ˜¯é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
        if hasattr(self, 'categories') and self.categories:
            categories_str = self.categories
        else:
            categories_str = os.getenv("CATEGORIES")

        if not categories_str:
            self.target_categories = set()
        else:
            self.target_categories = set(map(str.strip, categories_str.split(",")))
        
        # [REMOVED] start_urls - æ”¹ç”¨ start_requests() æ–¹æ³•ä»¥æ”¯æŒ errback
        # self.start_urls = [
        #     f"https://arxiv.org/list/{cat}/new" for cat in self.target_categories
        # ]
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            "total_found": 0,
            "unique_found": 0, # [NEW] å»é‡åçš„æ•°é‡
            "yielded": 0,
            "skipped_category": 0,
            "categories_found": set(),
            "all_subcategories": set(),  # [NEW] æ‰€æœ‰è®ºæ–‡çš„å­ç±»åˆ«æ ‡ç­¾ï¼ˆå»é‡ï¼‰
            "failed_categories": []  # [NEW] çˆ¬å–å¤±è´¥çš„ç±»åˆ«åˆ—è¡¨
        }
        self.seen_ids = set() # [NEW] ç”¨äºæœ¬æ¬¡çˆ¬å–ä¼šè¯å»é‡
        
        # æ•°æ®åº“è¿æ¥
        self.db = get_db()
        self.date_saved = False # æ ‡å¿—ä½ï¼Œé¿å…é‡å¤å†™å…¥æ—¥æœŸ
    
    def start_requests(self):
        """
        ç”Ÿæˆåˆå§‹è¯·æ±‚ã€‚
        
        ä¸ºæ¯ä¸ªç›®æ ‡ç±»åˆ«ç”Ÿæˆè¯·æ±‚ï¼Œå¹¶æ·»åŠ  errback é’©å­ä»¥æ•è·å¤±è´¥ã€‚
        
        Yields:
            scrapy.Request: å¸¦æœ‰ callback å’Œ errback çš„è¯·æ±‚å¯¹è±¡
        """
        for cat in self.target_categories:
            url = f"https://arxiv.org/list/{cat}/new"
            yield scrapy.Request(
                url=url,
                callback=self.parse,
                errback=self.handle_error,  # æ•è·æ‰€æœ‰ç±»å‹çš„è¯·æ±‚å¤±è´¥
                meta={'category': cat},  # ä¼ é€’ç±»åˆ«ä¿¡æ¯ç»™ errback
                dont_filter=True  # å…è®¸é‡è¯•æ—¶é‡å¤è¯·æ±‚åŒä¸€ä¸ª URL
            )

    def parse(self, response):
        self.logger.info(f"æ­£åœ¨è§£æé¡µé¢: {response.url}")
        
        # è·å–å½“å‰é¡µé¢çš„åˆ†ç±»
        current_category = response.url.split("/list/")[-1].split("/")[0]
        self.stats["categories_found"].add(current_category)
        
        # --- [NEW] ä¸¥æ ¼æ—¥æœŸè§£æ (Strict Date Parsing) ---
        if not self.date_saved:
            try:
                # å°è¯•æå–æ—¥æœŸæ–‡æœ¬
                # ç›®æ ‡æ ¼å¼: "Showing new listings for Monday, 15 December 2025"
                # XPath: //div[@id="dlpage"]/h3
                h3_text = response.xpath('//div[@id="dlpage"]/h3/text()').get()
                
                if not h3_text:
                    raise ValueError("æ— æ³•æ‰¾åˆ°åŒ…å«æ—¥æœŸçš„ h3 æ ‡ç­¾ (h3 tag not found)")
                
                # ä½¿ç”¨æ­£åˆ™æå–æ—¥æœŸéƒ¨åˆ†
                # åŒ¹é…æ¨¡å¼: æ˜ŸæœŸ, æ—¥ æœˆ å¹´ (e.g., Monday, 15 December 2025)
                match = re.search(r'listings for\s+(?:[a-zA-Z]+,\s+)?(\d{1,2}\s+[a-zA-Z]+\s+\d{4})', h3_text)
                
                if not match:
                    raise ValueError(f"æ—¥æœŸæ ¼å¼ä¸åŒ¹é… (Date format mismatch): {h3_text}")
                
                date_str = match.group(1)
                # è§£æä¸º datetime å¯¹è±¡
                # %d %B %Y -> 15 December 2025
                arxiv_date_obj = datetime.strptime(date_str, "%d %B %Y")
                arxiv_date_iso = arxiv_date_obj.strftime("%Y-%m-%d")
                
                self.logger.info(f"âœ… æˆåŠŸè§£æ ArXiv æ—¥æœŸ: {arxiv_date_iso} (from '{h3_text}')")
                
                # å­˜å…¥æ•°æ®åº“ system_status
                self.db.table("system_status").upsert({
                    "key": "latest_arxiv_date",
                    "value": arxiv_date_iso,
                    "updated_at": datetime.now().isoformat()
                }).execute()
                
                self.date_saved = True
                
            except Exception as e:
                error_msg = f"ğŸ›‘ ä¸¥é‡é”™è¯¯: ArXiv æ—¥æœŸè§£æå¤±è´¥ (Critical: Failed to parse ArXiv date).\nURL: {response.url}\nError: {str(e)}"
                self.logger.error(error_msg)
                
                # å‘é€æŠ¥è­¦é‚®ä»¶
                try:
                    email_sender.send_email(
                        to_email="2962326813@qq.com",
                        subject="ã€ç³»ç»ŸæŠ¥è­¦ã€‘ArXiv çˆ¬è™«æ—¥æœŸè§£æå¤±è´¥",
                        html_content=f"<p>{error_msg}</p>",
                        plain_content=error_msg
                    )
                except Exception as email_e:
                    self.logger.error(f"å‘é€æŠ¥è­¦é‚®ä»¶å¤±è´¥: {email_e}")
                
                # ä¸¥æ ¼æ¨¡å¼ï¼šåœæ­¢çˆ¬è™«
                raise CloseSpider(f"Date parsing failed: {str(e)}")
        # ------------------------------------------------
        
        # æå– "Replacements" çš„é”šç‚¹
        replacement_anchor = None
        for li in response.css("div[id=dlpage] ul li"):
            a_text = li.css("a::text").get()
            href = li.css("a::attr(href)").get()
            # Check for "Replacements" or "Replacement submissions"
            if a_text and "Replacement" in a_text and href and "item" in href:
                try:
                    replacement_anchor = int(href.split("item")[-1])
                    self.logger.info(f"Found Replacements anchor: {replacement_anchor}")
                    break
                except ValueError:
                    pass
        
        dt_elements = response.xpath('//dl[@id="articles"]/dt')
        dd_elements = response.xpath('//dl[@id="articles"]/dd')
        
        items_to_yield = []

        for dt, dd in zip(dt_elements, dd_elements):
            self.stats["total_found"] += 1
            
            # æå–ArXiv ID
            paper_anchor = dt.xpath('./a[@name]/@name').get()
            if paper_anchor and "item" in paper_anchor:
                try:
                    paper_id_num = int(paper_anchor.split("item")[-1])
                    # åªæœ‰å½“æ‰¾åˆ°äº† replacement_anchor ä¸”å½“å‰ ID å¤§äºç­‰äºå®ƒæ—¶æ‰è·³è¿‡
                    if replacement_anchor is not None and paper_id_num >= replacement_anchor:
                        self.logger.debug(f"Skipping replacement paper at anchor {paper_id_num}")
                        continue
                except ValueError:
                    pass

            arxiv_id_text = dt.xpath('.//a[@title="Abstract"]/text()').get()
            if not arxiv_id_text:
                continue
            
            arxiv_id = arxiv_id_text.replace("arXiv:", "").strip()
            self.logger.debug(f"å‘ç°è®ºæ–‡: {arxiv_id}")
            
            # [NEW] ç»Ÿè®¡å»é‡æ•°é‡
            if arxiv_id not in self.seen_ids:
                self.seen_ids.add(arxiv_id)
                self.stats["unique_found"] += 1
            
            # æå–æ‰€æœ‰åˆ†ç±» (Tags)
            # ç»“æ„: <div class="list-subjects">
            # <span class="primary-subject">Primary (cs.XX)</span>; Secondary (cs.YY)
            # </div>
            subjects_div = dd.xpath('.//div[contains(@class, "list-subjects")]')
            
            # æå–ä¸»ç±»åˆ« (Primary Subject)
            primary_subject_text = subjects_div.xpath('.//span[@class="primary-subject"]/text()').get()
            primary_category = None
            
            if primary_subject_text:
                # ä» "Computer Vision and Pattern Recognition (cs.CV)" æå– "cs.CV"
                match = re.search(r'\(([^\)]+)\)', primary_subject_text)
                if match:
                    primary_category = match.group(1)
            
            # æå–æ‰€æœ‰åˆ†ç±»æ ‡ç­¾ï¼ˆåŒ…æ‹¬ä¸»ç±»åˆ«å’Œæ¬¡ç±»åˆ«ï¼‰
            subjects_text = subjects_div.get()
            all_tags = []
            if subjects_text:
                # æå–æ‰€æœ‰æ‹¬å·å†…çš„åˆ†ç±»ä»£ç ï¼Œå¦‚ (cs.CV), (math.OT), (eess.IV)
                found_tags = re.findall(r'\(([\w\.-]+)\)', subjects_text)
                # ä½¿ç”¨ dict.fromkeys ä¿ç•™é¡ºåºå»é‡
                all_tags = list(dict.fromkeys(found_tags))
            
            # ç¡®ä¿ä¸»ç±»åˆ«åœ¨ tags åˆ—è¡¨çš„ç¬¬ä¸€ä½
            if primary_category and primary_category not in all_tags:
                all_tags.insert(0, primary_category)
            elif primary_category and primary_category in all_tags:
                # å°†ä¸»ç±»åˆ«ç§»åˆ°ç¬¬ä¸€ä½
                all_tags.remove(primary_category)
                all_tags.insert(0, primary_category)

            # [DISABLED] åˆ†ç±»è¿‡æ»¤å·²ç¦ç”¨ - è·å–æ‰€æœ‰è®ºæ–‡ï¼ˆReplacementsé™¤å¤–ï¼‰
            # åŸé€»è¾‘ï¼šåªä¿ç•™åˆ†ç±»æ ‡ç­¾ä¸ target_categories æœ‰äº¤é›†çš„è®ºæ–‡
            # if not any(tag in self.target_categories for tag in all_tags):
            #     self.logger.debug(f"è·³è¿‡ {arxiv_id} - åˆ†ç±»ä¸åŒ¹é… {all_tags}")
            #     self.stats["skipped_category"] += 1
            #     continue

            # æ„å»º Item
            item = PaperItem()
            item["id"] = arxiv_id
            item["category"] = all_tags 
            item["title"] = ""
            item["authors"] = []
            item["published_date"] = ""
            item["links"] = {}
            item["comment"] = ""
            
            items_to_yield.append(item)
            
            # [NEW] æ”¶é›†è¯¥è®ºæ–‡çš„æ‰€æœ‰å­ç±»åˆ«æ ‡ç­¾
            for tag in all_tags:
                self.stats["all_subcategories"].add(tag)
            
        # å¼€å§‹å†™å…¥æ•°æ®åº“ (å¯è§†åŒ–è¿›åº¦)
        from tqdm import tqdm
        import sys
        
        if items_to_yield:
            pbar = tqdm(
                total=len(items_to_yield), 
                desc=f"Saving {current_category} to DB", 
                unit="paper",
                file=sys.stdout,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            )
            
            for item in items_to_yield:
                self.stats["yielded"] += 1
                yield item
                pbar.update(1)
                
            pbar.close()
            
            # [NEW] å®æ—¶æ˜¾ç¤ºè¯¥é¡µé¢çš„çˆ¬å–è¿›åº¦
            print(f"âœ… {current_category}: æŠ“å–åˆ° {len(items_to_yield)} ç¯‡è®ºæ–‡")
        else:
            # [MODIFIED] æ˜¾ç¤ºæœªæ‰¾åˆ°è®ºæ–‡çš„æç¤º
            print(f"âš ï¸  {current_category}: æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
    
    def handle_error(self, failure):
        """
        å¤„ç†è¯·æ±‚å¤±è´¥ã€‚
        
        å½“è¯·æ±‚å› ç½‘ç»œé”™è¯¯ã€HTTPé”™è¯¯æˆ–å…¶ä»–åŸå› å¤±è´¥æ—¶ï¼Œæ­¤æ–¹æ³•ä¼šè¢«è°ƒç”¨ã€‚
        è®°å½•å¤±è´¥ä¿¡æ¯å¹¶æ‰“å°å‹å¥½æç¤ºï¼Œé¿å…æ•´ä¸ªçˆ¬å–æµç¨‹ä¸­æ–­ã€‚
        
        Args:
            failure: Scrapy çš„ Failure å¯¹è±¡ï¼ŒåŒ…å«é”™è¯¯ä¿¡æ¯
        """
        # ä» meta ä¸­è·å–ç±»åˆ«ä¿¡æ¯
        category = failure.request.meta.get('category', 'unknown')
        
        # è·å–é”™è¯¯è¯¦æƒ…
        error_msg = str(failure.value)
        
        # è®°å½•å¤±è´¥çš„ç±»åˆ«
        self.stats["failed_categories"].append({
            "category": category,
            "error": error_msg
        })
        
        # æ‰“å°å‹å¥½æç¤º
        print(f"âŒ {category}: çˆ¬å–å¤±è´¥ - {error_msg}")
        self.logger.error(f"Failed to crawl {category}: {failure}")

    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶è¾“å‡ºæ€»ç»“"""
        print("\n" + "="*50)
        print("ğŸ” ArXiv çˆ¬è™«æ‰§è¡Œæ€»ç»“")
        print("="*50)
        print(f"ğŸ“… ç›®æ ‡åˆ†ç±»: {', '.join(self.target_categories)}")
        print(f"ğŸ“‚ å®é™…æ‰«æåˆ†ç±»: {', '.join(self.stats['categories_found'])}")
        print(f"ğŸ“„ æ€»å…±å‘ç°è®ºæ–‡ (å»é‡å): {self.stats['unique_found']} (åŸå§‹æŠ“å–: {self.stats['total_found']})")
        print(f"âœ… æ•è·å¹¶æäº¤å¤„ç†: {self.stats['yielded']}")
        print(f"ğŸš« å› åˆ†ç±»ä¸ç¬¦è·³è¿‡: {self.stats['skipped_category']}")
        
        # [NEW] æ˜¾ç¤ºå¤±è´¥çš„ç±»åˆ«
        if self.stats['failed_categories']:
            print(f"âŒ çˆ¬å–å¤±è´¥çš„ç±»åˆ«: {len(self.stats['failed_categories'])} ä¸ª")
            for failed in self.stats['failed_categories']:
                print(f"   - {failed['category']}: {failed['error']}")
        
        # å°è¯•è·å– Pipeline çš„ç»Ÿè®¡ä¿¡æ¯ (å¦‚æœ Pipeline æ›´æ–°äº† crawler.stats)
        inserted = self.crawler.stats.get_value('papers/inserted', 0)
        duplicates = self.crawler.stats.get_value('papers/duplicates', 0)
        failed = self.crawler.stats.get_value('papers/failed', 0)
        
        if inserted or duplicates or failed:
            print("-" * 30)
            print("ğŸ’¾ æ•°æ®åº“å¤„ç†ç»“æœ:")
            print(f"   ğŸ†• æ–°å¢å…¥åº“: {inserted}")
            print(f"   â™»ï¸ é‡å¤/å·²å­˜åœ¨: {duplicates}")
            print(f"   âŒ å¤„ç†å¤±è´¥: {failed}")
        
        print("="*50 + "\n")
        
        # [NEW] è¾“å‡ºæœºå™¨å¯è¯»çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä¾› WorkflowStep è§£æ
        # å¿…é¡»ç¡®ä¿ unique_found æ˜¯å»é‡åçš„æ•°é‡
        import json
        # Convert sets to list for JSON serialization if any
        stats_serializable = self.stats.copy()
        if "categories_found" in stats_serializable:
            stats_serializable["categories_found"] = list(stats_serializable["categories_found"])
        # [NEW] è½¬æ¢ all_subcategories ä¸ºåˆ—è¡¨
        if "all_subcategories" in stats_serializable:
            stats_serializable["all_subcategories"] = list(stats_serializable["all_subcategories"])
            
        print(f"JSON_STATS:{json.dumps(stats_serializable)}")
