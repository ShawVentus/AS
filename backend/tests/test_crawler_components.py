import sys
import os
from pathlib import Path
import requests
import json
from datetime import datetime
from scrapy.http import HtmlResponse, Request
from dotenv import load_dotenv

# ==========================================
# ğŸ”§ æµ‹è¯•é…ç½® (Configuration)
# ==========================================
TARGET_CATEGORY = "cs.CL"       # ç›®æ ‡æµ‹è¯•ç±»åˆ«
TEST_BATCH_SIZE = 100             # é˜¶æ®µ 2 æµ‹è¯•çš„è®ºæ–‡æ•°é‡
REQUEST_TIMEOUT = 10            # ç½‘é¡µè¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)
# ==========================================

# æ·»åŠ  backend ç›®å½•åˆ° Python è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥ crawler æ¨¡å—
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

# å¯¼å…¥æˆ‘ä»¬çš„çˆ¬è™«ç»„ä»¶
from crawler.spiders.arxiv import ArxivSpider
from crawler.pipelines import ArxivApiPipeline
from crawler.items import PaperItem

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(backend_dir / ".env")

# ç»“æœå­˜å‚¨ç›®å½•
OUTPUT_DIR = backend_dir / "tests" / "crawler"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def save_json(data, filename):
    """ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶"""
    filepath = OUTPUT_DIR / filename
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {filepath}")

def test_spider_extraction():
    """
    æµ‹è¯•é˜¶æ®µ 1: éªŒè¯ Spider èƒ½å¦ä»ç½‘é¡µæ­£ç¡®æå– ID å’Œåˆ†ç±»
    """
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•é˜¶æ®µ 1: çˆ¬è™«ç½‘é¡µè§£æ (è·å– ID å’Œåˆ†ç±»)")
    print("="*50)

    # 1. æ¨¡æ‹Ÿè¯·æ±‚ arXiv åˆ—è¡¨é¡µ
    url = f"https://arxiv.org/list/{TARGET_CATEGORY}/new"
    print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚ç½‘é¡µ: {url} ...")
    
    try:
        # ä½¿ç”¨ requests è·å–çœŸå®ç½‘é¡µå†…å®¹
        response = requests.get(url, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        html_content = response.content
        print("âœ… ç½‘é¡µè·å–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç½‘é¡µè·å–å¤±è´¥: {e}")
        return []

    # 2. æ„å»º Scrapy Response å¯¹è±¡ (æ¨¡æ‹Ÿ Scrapy çš„å“åº”)
    scrapy_response = HtmlResponse(
        url=url,
        body=html_content,
        encoding='utf-8',
        request=Request(url=url)
    )

    # 3. å®ä¾‹åŒ– Spider å¹¶è°ƒç”¨ parse æ–¹æ³•
    spider = ArxivSpider()
    print("ğŸ•·ï¸  æ­£åœ¨è¿è¡Œ Spider.parse() ...")
    
    extracted_items = []
    # parse æ–¹æ³•è¿”å›çš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬éœ€è¦éå†å®ƒ
    for item in spider.parse(scrapy_response):
        if isinstance(item, PaperItem):
            # å°† Item è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿åºåˆ—åŒ–
            extracted_items.append(dict(item))
    
    # 4. éªŒè¯ç»“æœå¹¶ä¿å­˜
    print(f"ğŸ“Š æå–ç»“æœ: æ‰¾åˆ° {len(extracted_items)} ç¯‡è®ºæ–‡")
    
    # ä¿å­˜é˜¶æ®µ 1 ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_json(extracted_items, f"stage1_spider_ids_{timestamp}.json")
    
    if len(extracted_items) > 0:
        first_item = extracted_items[0]
        print(f"\nğŸ“ ç¤ºä¾‹æ•°æ® (ç¬¬ä¸€ç¯‡):")
        print(f"   - ID: {first_item.get('id')}")
        print(f"   - Category: {first_item.get('category')}")
        print("âœ… é˜¶æ®µ 1 æµ‹è¯•é€šè¿‡ï¼")
        return extracted_items
    else:
        print("âŒ é˜¶æ®µ 1 æµ‹è¯•å¤±è´¥: æœªæå–åˆ°ä»»ä½•è®ºæ–‡")
        return []

def test_api_enrichment(raw_items):
    """
    æµ‹è¯•é˜¶æ®µ 2: éªŒè¯ Pipeline èƒ½å¦é€šè¿‡ API è¡¥å…¨è®ºæ–‡ä¿¡æ¯
    """
    print("\n" + "="*50)
    print(f"ğŸ§ª æµ‹è¯•é˜¶æ®µ 2: API å…ƒæ•°æ®è·å– (æµ‹è¯•å‰ {min(len(raw_items), TEST_BATCH_SIZE)} ç¯‡)")
    print("="*50)
    
    if not raw_items:
        print("âš ï¸  è·³è¿‡é˜¶æ®µ 2: å› ä¸ºé˜¶æ®µ 1 æœªè¿”å›æœ‰æ•ˆæ•°æ®")
        return

    # 1. å®ä¾‹åŒ– Pipeline
    pipeline = ArxivApiPipeline()
    print("ğŸ”§ Pipeline åˆå§‹åŒ–å®Œæˆ")

    # æ¨¡æ‹Ÿ Spider å¯¹è±¡ (Pipeline éœ€è¦ç”¨åˆ° spider.logger)
    class MockSpider:
        class Logger:
            def info(self, msg): print(f"   [INFO] {msg}")
            def warning(self, msg): print(f"   [WARN] {msg}")
            def error(self, msg): print(f"   [ERROR] {msg}")
        logger = Logger()
    
    mock_spider = MockSpider()

    enriched_results = []
    
    # 2. æ‰¹é‡å¤„ç†
    items_to_process = raw_items[:TEST_BATCH_SIZE]
    
    for i, raw_item in enumerate(items_to_process, 1):
        print(f"\n[{i}/{len(items_to_process)}] ğŸ”„ æ­£åœ¨å¤„ç†è®ºæ–‡ ID: {raw_item['id']} ...")
        
        try:
            # process_item ä¼šä¿®æ”¹ä¼ å…¥çš„ itemï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªå‰¯æœ¬
            item_to_process = raw_item.copy()
            enriched_item = pipeline.process_item(item_to_process, mock_spider)
            enriched_results.append(dict(enriched_item))
            
            # ç®€å•éªŒè¯
            if enriched_item.get('title'):
                print(f"   âœ… è·å–æˆåŠŸ: {enriched_item.get('title')[:50]}...")
            else:
                print("   âŒ è·å–å¤±è´¥: æ ‡é¢˜ä¸ºç©º")
                
        except Exception as e:
            print(f"   âŒ å¤„ç†å‡ºé”™: {e}")

    # 3. ä¿å­˜ç»“æœ
    if enriched_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_json(enriched_results, f"stage2_api_details_{timestamp}.json")
        print("\nâœ… é˜¶æ®µ 2 æµ‹è¯•å®Œæˆï¼")
    else:
        print("\nâŒ é˜¶æ®µ 2 æµ‹è¯•å¤±è´¥: æœªè·å–åˆ°ä»»ä½•ç»“æœ")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è¿è¡Œçˆ¬è™«ç»„ä»¶æµ‹è¯•...")
    print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: {OUTPUT_DIR}")
    print(f"âš™ï¸  é…ç½®: Category={TARGET_CATEGORY}, BatchSize={TEST_BATCH_SIZE}")
    
    # è¿è¡Œé˜¶æ®µ 1
    items = test_spider_extraction()
    
    # è¿è¡Œé˜¶æ®µ 2
    if items:
        test_api_enrichment(items)
    else:
        # å¦‚æœé˜¶æ®µ 1 å¤±è´¥ï¼Œä½¿ç”¨ç¡¬ç¼–ç æ•°æ®æµ‹è¯•
        print("\nâš ï¸  å°è¯•ä½¿ç”¨ç¡¬ç¼–ç  ID è¿›è¡Œé˜¶æ®µ 2 æµ‹è¯•...")
        dummy_items = [{
            'id': "1706.03762",
            'category': ["cs.CL"],
            'title': "",
            'authors': [],
            'published_date': "",
            'tldr': "",
            'details': {},
            'links': {},
            'comment': ""
        }]
        test_api_enrichment(dummy_items)
