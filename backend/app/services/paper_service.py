from typing import List, Optional, Union, Callable, Dict, Any
from datetime import datetime, timedelta
import json
import subprocess
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm # å¼•å…¥ tqdm ç”¨äºè¿›åº¦æ¡
from app.schemas.paper import RawPaperMetadata, UserPaperState, PersonalizedPaper, PaperAnalysis, PaperDetails, PaperLinks, PaperFilter, FilterResponse, FilterResultItem, PaperExportRequest
from app.schemas.user import UserProfile
from app.services.llm_service import llm_service
from app.core.database import get_db

# ä»ç¯å¢ƒå˜é‡è·å–æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä¸º 2
MAX_WORKERS = int(os.getenv("LLM_MAX_WORKERS", 2))

class PaperService:
    def __init__(self):
        self.db = get_db()
        # åŠ è½½ç±»åˆ«æ˜ å°„è¡¨
        self._load_category_mapping()
    
    def _format_preferences_for_llm(self, preferences: List[str]) -> str:
        """
        å°† preferences åˆ—è¡¨æ ¼å¼åŒ–ä¸º LLM å‹å¥½çš„ Markdown æ ¼å¼
        
        å°†ç”¨æˆ·çš„ç ”ç©¶åå¥½åˆ—è¡¨è½¬æ¢ä¸º Markdown æ— åºåˆ—è¡¨æ ¼å¼ï¼Œä¾¿äº LLM ç†è§£å’Œå¤„ç†ã€‚
        
        Args:
            preferences (List[str]): ç”¨æˆ·çš„ç ”ç©¶åå¥½åˆ—è¡¨
        
        Returns:
            str: Markdown æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²ï¼Œå¦‚æœåˆ—è¡¨ä¸ºç©ºåˆ™è¿”å›æç¤ºæ–‡æœ¬
            
        Example:
            è¾“å…¥: ["å¯»æ‰¾çµæ„Ÿ", "è·Ÿè¿›å‰æ²¿"]
            è¾“å‡º: "- å¯»æ‰¾çµæ„Ÿ\n- è·Ÿè¿›å‰æ²¿"
        """
        if preferences and len(preferences) > 0:
            # æ ¼å¼åŒ–ä¸º Markdown æ— åºåˆ—è¡¨
            formatted = "\n".join([f"- {pref}" for pref in preferences])
            print(f"[LLMç­›é€‰] æ ¼å¼åŒ– preferences: {len(preferences)} æ¡")
            return formatted
        else:
            # ç©ºåˆ—è¡¨ï¼Œè¿”å›æç¤ºæ–‡æœ¬
            return "ï¼ˆç”¨æˆ·æœªè®¾ç½®ç ”ç©¶åå¥½ï¼‰"
    
    def _load_category_mapping(self):
        """
        åŠ è½½ arXiv ç±»åˆ«æ˜ å°„è¡¨ã€‚
        
        ä» frontend/src/assets/arxiv_category_simple.json åŠ è½½ç±»åˆ«æ˜ å°„å…³ç³»ï¼Œ
        ç”¨äºå°†ä¸»ç±»åˆ«ï¼ˆå¦‚ 'stat'ã€'q-fin'ï¼‰å±•å¼€ä¸ºæ‰€æœ‰å­ç±»åˆ«ã€‚
        
        Args:
            None
            
        Returns:
            None
        """
        try:
            # æ„é€  JSON æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            # backend/app/services/paper_service.py -> backend/
            # ç„¶åå‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œå†å®šä½åˆ° frontend/src/assets/
            current_dir = os.path.dirname(os.path.abspath(__file__))
            backend_root = os.path.dirname(os.path.dirname(current_dir))
            project_root = os.path.dirname(backend_root)
            json_path = os.path.join(project_root, "frontend", "src", "assets", "arxiv_category_simple.json")
            
            with open(json_path, 'r', encoding='utf-8') as f:
                self.category_mapping = json.load(f)
            
            print(f"[ç±»åˆ«æ˜ å°„] å·²åŠ è½½ {len(self.category_mapping)} ä¸ªä¸»ç±»åˆ«çš„æ˜ å°„è¡¨")
        except Exception as e:
            print(f"[ç±»åˆ«æ˜ å°„] åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç©ºæ˜ å°„è¡¨")
            self.category_mapping = {}
    
    def expand_categories(self, categories: List[str]) -> List[str]:
        """
        å±•å¼€ç±»åˆ«åˆ—è¡¨ï¼Œå°†ä¸»ç±»åˆ«è½¬æ¢ä¸ºæ‰€æœ‰å­ç±»åˆ«ã€‚
        
        åŠŸèƒ½ï¼š
            1. éå†è¾“å…¥çš„ç±»åˆ«åˆ—è¡¨
            2. å¦‚æœæ˜¯ä¸»ç±»åˆ«ï¼ˆå­˜åœ¨äºæ˜ å°„è¡¨ä¸­ï¼‰ï¼Œå±•å¼€ä¸ºæ‰€æœ‰å­ç±»åˆ«
            3. å¦‚æœå·²ç»æ˜¯å­ç±»åˆ«ï¼Œä¿æŒä¸å˜
            4. è¿”å›å±•å¼€åçš„å®Œæ•´ç±»åˆ«åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        
        Args:
            categories (List[str]): è¾“å…¥çš„ç±»åˆ«åˆ—è¡¨ï¼Œå¯èƒ½åŒ…å«ä¸»ç±»åˆ«æˆ–å­ç±»åˆ«
            
        Returns:
            List[str]: å±•å¼€åçš„ç±»åˆ«åˆ—è¡¨
            
        Example:
            è¾“å…¥: ['stat', 'q-fin', 'cs.AI']
            è¾“å‡º: ['stat.AP', 'stat.CO', 'stat.ML', 'stat.ME', 'stat.OT', 'math.ST',
                   'q-fin.CP', 'econ.GN', 'q-fin.GN', ..., 'cs.AI']
        """
        expanded = []
        
        for cat in categories:
            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸»ç±»åˆ«ï¼ˆåœ¨æ˜ å°„è¡¨çš„é”®ä¸­ï¼‰
            if cat in self.category_mapping:
                # å±•å¼€ä¸ºæ‰€æœ‰å­ç±»åˆ«
                subcategories = self.category_mapping[cat]
                expanded.extend(subcategories)
                print(f"[ç±»åˆ«å±•å¼€] '{cat}' -> {len(subcategories)} ä¸ªå­ç±»åˆ«")
            else:
                # å·²ç»æ˜¯å­ç±»åˆ«æˆ–æœªçŸ¥ç±»åˆ«ï¼Œç›´æ¥æ·»åŠ 
                expanded.append(cat)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        expanded = list(dict.fromkeys(expanded))
        
        if len(expanded) != len(categories):
            print(f"[ç±»åˆ«å±•å¼€] è¾“å…¥ {len(categories)} ä¸ªç±»åˆ«ï¼Œå±•å¼€ä¸º {len(expanded)} ä¸ªç±»åˆ«")
        
        return expanded


    def clear_daily_papers(self) -> bool:
        """
        æ¸…ç©ºæ¯æ—¥æ›´æ–°æ•°æ®åº“ã€‚
        """
        try:
            # delete all rows
            self.db.table("daily_papers").delete().neq("id", "00000").execute()
            return True
        except Exception as e:
            print(f"Error clearing daily papers: {e}")
            return False

    def archive_daily_papers(self) -> bool:
        """
        å°† daily_papers ä¸­çš„æ•°æ®å½’æ¡£åˆ° papers è¡¨ã€‚
        ä¿ç•™ daily_papers ä¸­çš„æ•°æ®ã€‚
        
        Args:
            None
            
        Returns:
            bool: å½’æ¡£æ˜¯å¦æˆåŠŸã€‚
        """
        print("Starting archiving daily papers to public DB...")
        try:
            # 1. è·å–æ‰€æœ‰ daily_papers
            # å‡è®¾æ•°é‡ä¸å¤§ï¼Œä¸€æ¬¡æ€§è·å–ã€‚å¦‚æœæ•°é‡å¤§éœ€è¦åˆ†é¡µã€‚
            response = self.db.table("daily_papers").select("*").execute()
            daily_papers = response.data
            
            if not daily_papers:
                print("No papers in daily_papers to archive.")
                return True
                
            print(f"Found {len(daily_papers)} papers to archive.")
            
            # 2. æ‰¹é‡æ’å…¥/æ›´æ–°åˆ° papers è¡¨
            # Supabase upsert
            res = self.db.table("papers").upsert(daily_papers).execute()
            
            print(f"Successfully archived {len(res.data)} papers.")
            return True
            
        except Exception as e:
            print(f"Error archiving papers: {e}")
            return False

    def export_papers(self, request: PaperExportRequest) -> Union[str, List[dict]]:
        """
        å¯¼å‡ºè®ºæ–‡åŠŸèƒ½ã€‚
        æ ¹æ®ç”¨æˆ·è¯·æ±‚çš„æ¡ä»¶ï¼Œä»æ•°æ®åº“ä¸­ç­›é€‰å¹¶å¯¼å‡ºè®ºæ–‡ã€‚

        Args:
            request (PaperExportRequest): å¯¼å‡ºè¯·æ±‚å¯¹è±¡ï¼ŒåŒ…å«ç­›é€‰æ¡ä»¶ã€‚
                - user_id: ç”¨æˆ·ID
                - date_start: å¼€å§‹æ—¥æœŸï¼ˆå¿…å¡«ï¼Œæ ¼å¼ï¼šYYYY-MM-DDï¼‰
                - date_end: ç»“æŸæ—¥æœŸï¼ˆå¿…å¡«ï¼Œæ ¼å¼ï¼šYYYY-MM-DDï¼‰
                - limit: å¯¼å‡ºæ•°é‡é™åˆ¶
                - format: è¾“å‡ºæ ¼å¼ï¼ˆmarkdown/jsonï¼‰
                - min_score: æœ€ä½ç›¸å…³æ€§è¯„åˆ†ï¼ˆå¯é€‰ï¼‰

        Returns:
            str | List[dict]: å¦‚æœæ ¼å¼ä¸º markdownï¼Œè¿”å›å­—ç¬¦ä¸²ï¼›å¦‚æœä¸º jsonï¼Œè¿”å›å­—å…¸åˆ—è¡¨ã€‚
        """
        try:
            print(f"å¼€å§‹å¯¼å‡ºè®ºæ–‡: user_id={request.user_id}, æ—¥æœŸèŒƒå›´={request.date_start} è‡³ {request.date_end}")
            
            # 1. æ„å»ºæŸ¥è¯¢ user_paper_statesï¼ˆæ—¶é—´èŒƒå›´ä¸ºå¿…å¡«ï¼‰
            query = (self.db.table("user_paper_states")
                    .select("*")
                    .eq("user_id", request.user_id)
                    .gte("created_at", request.date_start)
                    .lte("created_at", f"{request.date_end} 23:59:59"))

            # è¯„åˆ†ç­›é€‰ï¼ˆå¯é€‰ï¼‰
            if request.min_score is not None:
                query = query.gte("relevance_score", request.min_score)

            # æ’åºå¹¶é™åˆ¶æ•°é‡
            query = query.order("relevance_score", desc=True).limit(request.limit)
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = query.execute()
            states = response.data
            
            if not states:
                print("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
                return "" if request.format == "markdown" else []

            print(f"æ‰¾åˆ° {len(states)} ç¯‡ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
            paper_ids = [s["paper_id"] for s in states]
            
            # 2. è·å– papers è¯¦æƒ…
            papers_resp = self.db.table("papers").select("*").in_("id", paper_ids).execute()
            papers_map = {p["id"]: p for p in papers_resp.data}

            # 3. åˆå¹¶æ•°æ®
            results = []
            for state in states:
                p_data = papers_map.get(state["paper_id"])
                if not p_data:
                    print(f"âš ï¸  è­¦å‘Š: åœ¨ papers è¡¨ä¸­æœªæ‰¾åˆ°è®ºæ–‡ ID: {state['paper_id']}ï¼Œè·³è¿‡")
                    continue
                
                # åˆå¹¶ä¿¡æ¯ï¼ˆä»…è¾“å‡ºéœ€è¦çš„å­—æ®µï¼‰
                paper_info = {
                    "id": p_data.get("id"),
                    "title": p_data.get("title"),
                    "authors": p_data.get("authors"),
                    "published_date": p_data.get("published_date"),
                    "abstract": p_data.get("abstract"),
                    "comment": p_data.get("comment"),
                    "why_this_paper": state.get("why_this_paper"),
                }
                results.append(paper_info)
            
            print(f"æˆåŠŸå¯¼å‡º {len(results)} ç¯‡è®ºæ–‡")
            
            # 4. æ ¼å¼åŒ–è¾“å‡º
            if request.format == "json":
                return results
            else:
                return self._format_as_markdown(results)

        except Exception as e:
            print(f"å¯¼å‡ºè®ºæ–‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return "" if request.format == "markdown" else []

    def _format_as_markdown(self, papers: List[dict]) -> str:
        """
        å°†è®ºæ–‡åˆ—è¡¨æ ¼å¼åŒ–ä¸º Markdown æ–‡æœ¬ã€‚

        Args:
            papers (List[dict]): è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ã€‚

        Returns:
            str: Markdown æ ¼å¼çš„æ–‡æœ¬ã€‚
        """
        md_lines = []
        for i, p in enumerate(papers, 1):
            # æ·»åŠ ç©ºå€¼ä¿æŠ¤
            title = p.get('title') or 'æ— æ ‡é¢˜'
            paper_id = p.get('id') or 'æœªçŸ¥'
            authors = p.get('authors', [])
            if isinstance(authors, list) and authors:
                authors_str = ', '.join(authors)
            else:
                authors_str = 'æœªçŸ¥ä½œè€…'
            
            why_this_paper = p.get('why_this_paper') or 'æ— æ¨èç†ç”±'
            abstract = p.get('abstract') or 'æ— æ‘˜è¦'
            
            md_lines.append(f"## {i}. {title}")
            md_lines.append(f"**ID**: {paper_id}")
            md_lines.append(f"**ä½œè€…**: {authors_str}")
            md_lines.append(f"**æ¨èç†ç”±**: {why_this_paper}")
            md_lines.append(f"**æ‘˜è¦**: {abstract}")
            
            # comment æ˜¯å¯é€‰å­—æ®µ
            if p.get('comment'):
                md_lines.append(f"**å¤‡æ³¨**: {p.get('comment')}")
            
            md_lines.append("---")
        return "\n".join(md_lines)

    def merge_paper_state(self, paper: dict, state: Optional[dict]) -> PersonalizedPaper:
        """
        å°†è®ºæ–‡å…ƒæ•°æ®ä¸ç”¨æˆ·çŠ¶æ€åˆå¹¶ (æ„é€ åµŒå¥—ç»“æ„)ã€‚
        
        Args:
            paper (dict): åŸå§‹è®ºæ–‡æ•°æ®å­—å…¸ã€‚
            state (Optional[dict]): ç”¨æˆ·çŠ¶æ€æ•°æ®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸º Noneã€‚

        Returns:
            PersonalizedPaper: åˆå¹¶åçš„ä¸ªæ€§åŒ–è®ºæ–‡å¯¹è±¡ã€‚
        """
        # 1. Construct Meta
        meta_data = {
            "id": paper["id"],
            "title": paper["title"],
            "authors": paper["authors"],
            "published_date": paper["published_date"],
            "created_at": paper.get("created_at"), # Map created_at
            "category": paper["category"],
            "abstract": paper["abstract"],
            "links": paper["links"],
            "comment": paper.get("comment")
        }
        meta = RawPaperMetadata(**meta_data)

        # 2. Construct Analysis
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†ææ•°æ® (details æˆ– tldr)
        analysis = None
        if paper.get("details") or paper.get("tldr"):
            details = paper.get("details") or {}
            analysis_data = {
                "tldr": details.get("tldr"),
                "tags": details.get("tags") or {},
                "motivation": details.get("motivation"),
                "method": details.get("method"),
                "result": details.get("result"),
                "conclusion": details.get("conclusion")
            }
            analysis = PaperAnalysis(**analysis_data)

        # 3. Construct User State
        user_state = UserPaperState(**state) if state else None

        return PersonalizedPaper(meta=meta, analysis=analysis, user_state=user_state)

    def get_papers_by_categories(self, categories: List[str], user_id: str, limit: int = 1, table_name: str = "papers", force: bool = False, published_date: Optional[str] = None) -> List[PersonalizedPaper]:
        """
        æ ¹æ®ç”¨æˆ·å…³æ³¨çš„ç±»åˆ«è·å–å€™é€‰è®ºæ–‡ã€‚
        æ’é™¤å·²åœ¨ user_paper_states ä¸­å­˜åœ¨çš„è®ºæ–‡ã€‚
        
        Args:
            categories (List[str]): ç”¨æˆ·å…³æ³¨çš„ç±»åˆ«åˆ—è¡¨ã€‚
            user_id (str): ç”¨æˆ· IDã€‚
            limit (int): é™åˆ¶æ•°é‡ã€‚
            table_name (str): è¡¨åã€‚
            force (bool): æ˜¯å¦å¼ºåˆ¶è·å–ï¼ˆå¿½ç•¥å·²å­˜åœ¨çš„çŠ¶æ€ï¼‰ã€‚

        Returns:
            List[PersonalizedPaper]: å€™é€‰è®ºæ–‡åˆ—è¡¨ã€‚
        """
        try:
            if not categories:
                return []

            # 1. è·å–ç”¨æˆ·å·²çœŸæ­£å¤„ç†è¿‡çš„ paper_ids (æ’é™¤ "Not Filtered" çŠ¶æ€çš„è®ºæ–‡)
            existing_ids = []
            if not force:
                existing_states = self.db.table("user_paper_states")\
                    .select("paper_id")\
                    .eq("user_id", user_id)\
                    .neq("why_this_paper", "Not Filtered")\
                    .execute()
                existing_ids = [row['paper_id'] for row in existing_states.data] if existing_states.data else []

            # 2. æŸ¥è¯¢ç¬¦åˆç±»åˆ«çš„è®ºæ–‡
            # ä½¿ç”¨ overlaps (æ•°ç»„é‡å ) åŒ¹é…ç±»åˆ«
            # æ³¨æ„: Supabase (PostgREST) çš„ overlaps è¯­æ³•æ˜¯ cs (contains) æˆ– cd (contained by) æˆ– ov (overlap)
            # è¿™é‡Œå‡è®¾ category å­—æ®µæ˜¯ text[] ç±»å‹
            print(f"[DEBUG] æŸ¥è¯¢å‚æ•°: table={table_name}, categories={categories}, published_date={published_date}, limit={limit}")
            query = self.db.table(table_name).select("*").overlaps("category", categories).order("created_at", desc=True).limit(limit)
            
            # [NEW] Date Filtering
            if published_date:
                query = query.eq("published_date", published_date)
            
            # æ’é™¤å·²å­˜åœ¨çš„
            if existing_ids:
                # not_in è¿‡æ»¤å™¨
                # query = query.not_.in_("id", existing_ids) # Supabase Python client è¯­æ³•å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œé€šå¸¸æ˜¯ .not_in
                # ç®€å•èµ·è§ï¼Œå¦‚æœä¸æ”¯æŒå¤æ‚é“¾å¼ï¼Œå¯ä»¥åœ¨å†…å­˜ä¸­è¿‡æ»¤ï¼Œæˆ–è€…åˆ†æ‰¹æŸ¥è¯¢
                # å°è¯•ä½¿ç”¨ filter
                pass 

            response = query.execute()
            papers_data = response.data if response.data else []
            print(f"[DEBUG] æŸ¥è¯¢è¿”å› {len(papers_data)} ç¯‡è®ºæ–‡")

            # å†…å­˜è¿‡æ»¤ (ä¸ºäº†ä¿é™©èµ·è§ï¼Œæˆ–è€…å¦‚æœ not_in è¯­æ³•æœ‰é—®é¢˜)
            candidates = []
            for p in papers_data:
                if force or p['id'] not in existing_ids:
                    # æ„é€ é»˜è®¤çŠ¶æ€ (None)
                    candidates.append(self.merge_paper_state(p, None))
            
            return candidates

        except Exception as e:
            print(f"Error fetching papers by categories: {e}")
            return []

    def update_user_feedback(self, user_id: str, paper_id: str, liked: Optional[bool], feedback: Optional[str], note: Optional[str] = None) -> bool:
        """
        æ›´æ–°ç”¨æˆ·å¯¹è®ºæ–‡çš„åé¦ˆ (Like/Dislike, Reason, Note)ã€‚å­˜å‚¨åˆ°æ•°æ®åº“å¯¹åº”å­—æ®µä¸­

        Args:
            user_id (str): ç”¨æˆ· IDã€‚
            paper_id (str): è®ºæ–‡ IDã€‚
            liked (Optional[bool]): æ˜¯å¦å–œæ¬¢ã€‚
            feedback (Optional[str]): åé¦ˆå†…å®¹ã€‚
            note (Optional[str]): ç”¨æˆ·ç¬”è®°ã€‚

        Returns:
            bool: æ˜¯å¦æ›´æ–°æˆåŠŸã€‚
        """
        try:
            # æ„é€ æ›´æ–°æ•°æ®
            update_data = {
                "user_id": user_id,
                "paper_id": paper_id,
                # ä»…æ›´æ–°éç©ºå­—æ®µï¼Œæˆ–è€…å…¨éƒ¨æ›´æ–°ï¼Ÿé€šå¸¸æ˜¯ patch æ›´æ–°
            }
            if liked is not None:
                update_data["user_liked"] = liked
            if feedback is not None:
                update_data["user_feedback"] = feedback
            if note is not None:
                update_data["note"] = note
            
            # ä½¿ç”¨ upsertï¼Œç¡®ä¿å¦‚æœè®°å½•ä¸å­˜åœ¨ä¹Ÿèƒ½åˆ›å»º (è™½ç„¶ç†è®ºä¸Šåº”è¯¥å…ˆæœ‰ filter è®°å½•)
            # ä½†ä¸ºäº†å¥å£®æ€§ï¼Œå¦‚æœç”¨æˆ·ç›´æ¥å¯¹æŸç¯‡æœª filter çš„è®ºæ–‡æ“ä½œ (æå°‘æƒ…å†µ)ï¼Œä¹Ÿèƒ½è®°å½•
            self.db.table("user_paper_states").upsert(update_data).execute()
            return True
        except Exception as e:
            print(f"Error updating user feedback: {e}")
            return False

    def get_papers(self, user_id: str) -> List[PersonalizedPaper]:
        """
        ä»æ•°æ®åº“è·å–æ‰€æœ‰è®ºæ–‡ï¼Œå¹¶é™„åŠ æŒ‡å®šç”¨æˆ·çš„çŠ¶æ€ä¿¡æ¯ã€‚

        Args:
            user_id (str): ç”¨æˆ· IDã€‚

        Returns:
            List[PersonalizedPaper]: ä¸ªæ€§åŒ–è®ºæ–‡åˆ—è¡¨ã€‚
        """
        try:
            # 1. Fetch Papers
            response = self.db.table("papers").select("*").order("created_at", desc=True).limit(50).execute()
            papers_data = response.data if response.data else []
            
            # 2. Fetch User States
            states_map = {}
            if user_id:
                state_response = self.db.table("user_paper_states").select("*").eq("user_id", user_id).execute()
                if state_response.data:
                    for s in state_response.data:
                        states_map[s['paper_id']] = s

            # 3. Merge
            results = []
            for p in papers_data:
                try:
                    state = states_map.get(p['id'])
                    # If no state exists, create a default one (in memory only)
                    if not state:
                        state = {
                            "paper_id": p['id'],
                            "user_id": user_id,
                            "relevance_score": 0.0,
                            "why_this_paper": "Not Filtered", # Default
                            "accepted": False,
                            "user_accepted": False,
                            "user_liked": None,
                            "user_feedback": None,
                            "note": None
                        }
                    results.append(self.merge_paper_state(p, state))
                except Exception as validation_error:
                    print(f"âš ï¸ Skipping invalid paper {p.get('id')}: {validation_error}")
            return results
        except Exception as e:
            print(f"Error fetching papers: {e}")
            return []

    def crawl_arxiv_new(self, user_id: str, limit: int = 100) -> List[PersonalizedPaper]:
        """
        è§¦å‘çˆ¬è™«æŠ“å–æœ€æ–°çš„ Arxiv è®ºæ–‡ã€‚

        Args:
            user_id (str): ç”¨æˆ· IDã€‚
            limit (int, optional): æŠ“å–é™åˆ¶æ•°é‡ã€‚é»˜è®¤ä¸º 100ã€‚

        Returns:
            List[PersonalizedPaper]: æŠ“å–åæœ€æ–°çš„è®ºæ–‡åˆ—è¡¨ã€‚
        """
        try:
            backend_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
            subprocess.run(["scrapy", "crawl", "arxiv"], check=True, cwd=backend_root)
            return self.get_papers(user_id)
        except Exception as e:
            print(f"Error crawling: {e}")
            return self.get_papers(user_id)

    def process_pending_papers(self, user_id: str, progress_callback: Optional[Callable[[int, int, str], None]] = None, manual_query: Optional[str] = None, manual_authors: Optional[List[str]] = None, manual_categories: Optional[List[str]] = None, force: bool = False, published_date: Optional[str] = None) -> FilterResponse:
        """
        å¤„ç†ç”¨æˆ·çš„å¾…å¤„ç†è®ºæ–‡ (Pending Papers)ã€‚
        
        æµç¨‹:
        1. è·å–ç”¨æˆ·ç”»åƒ (Profile)ã€‚
        2. æ ¹æ®ç”»åƒä¸­çš„å…³æ³¨ç±»åˆ« (Focus.category) æˆ–æ‰‹åŠ¨è¾“å…¥çš„ç±»åˆ«è·å–å€™é€‰è®ºæ–‡ã€‚
        3. è°ƒç”¨ filter_papers è¿›è¡Œæ‰¹é‡ç­›é€‰ (LLM)ã€‚
        
        Args:
            user_id (str): ç”¨æˆ· IDã€‚
            progress_callback (Optional[Callable]): è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç”¨äºæ›´æ–°ä»»åŠ¡è¿›åº¦ã€‚
            manual_query (Optional[str]): æ‰‹åŠ¨è¾“å…¥çš„è‡ªç„¶è¯­è¨€éœ€æ±‚ (è¦†ç›–ç”¨æˆ·ç”»åƒä¸­çš„æè¿°)ã€‚
            manual_authors (Optional[List[str]]): æ‰‹åŠ¨è¾“å…¥çš„ä½œè€…åˆ—è¡¨ (è¦†ç›–ç”¨æˆ·ç”»åƒä¸­çš„ä½œè€…)ã€‚
            manual_categories (Optional[List[str]]): æ‰‹åŠ¨è¾“å…¥çš„ç±»åˆ«åˆ—è¡¨ (è¦†ç›–ç”¨æˆ·ç”»åƒä¸­çš„ç±»åˆ«)ã€‚
            
        Returns:
            FilterResponse: ç­›é€‰ç»“æœç»Ÿè®¡å¯¹è±¡ï¼ŒåŒ…å«å·²æ¥å—ã€å·²æ‹’ç»çš„è®ºæ–‡åˆ—è¡¨åŠç»Ÿè®¡ä¿¡æ¯ã€‚
        """
        try:
            # 1. è·å–ç”¨æˆ·ç”»åƒ
            # å±€éƒ¨å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
            from app.services.user_service import user_service
            profile = user_service.get_profile(user_id)
            print(f"Start processing pending papers for user: {profile.info.name}")
            
            # 2. è·å–å€™é€‰è®ºæ–‡
            # ä¼˜å…ˆä½¿ç”¨æ‰‹åŠ¨è¾“å…¥çš„ç±»åˆ«ï¼Œå¦åˆ™ä½¿ç”¨ç”¨æˆ·ç”»åƒä¸­çš„ç±»åˆ«
            categories = manual_categories if manual_categories else profile.focus.category
            
            if not categories:
                print(f"User {profile.info.name} ({user_id}) has no focus categories (and no manual categories provided).")
                # è¿”å›ç©ºç»“æœ
                from app.schemas.paper import FilterResponse
                from datetime import datetime
                return FilterResponse(
                    user_id=user_id,
                    created_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    total_analyzed=0,
                    accepted_count=0,
                    rejected_count=0,
                    selected_papers=[],
                    rejected_papers=[]
                )
            
            # [æ–°å¢] å±•å¼€ç±»åˆ«ï¼šå°†ä¸»ç±»åˆ«è½¬æ¢ä¸ºæ‰€æœ‰å­ç±»åˆ«
            # ä¾‹å¦‚: ['stat', 'q-fin'] -> ['stat.AP', 'stat.CO', ..., 'q-fin.CP', ...]
            expanded_categories = self.expand_categories(categories)
            print(f"[ç±»åˆ«å¤„ç†] åŸå§‹ç±»åˆ«: {categories}")
            print(f"[ç±»åˆ«å¤„ç†] å±•å¼€åç±»åˆ«: {expanded_categories}")

            # è·å–æœªå¤„ç†çš„è®ºæ–‡
            # ä¼˜å…ˆä» daily_papers è·å– (å½“æ—¥æ›´æ–°)
            # å¦‚æœ daily_papers ä¸ºç©º (ä¾‹å¦‚ä»Šå¤©æ²¡æœ‰æ›´æ–°)ï¼Œåˆ™é€»è¾‘ä¸Šåº”è¯¥è¿”å›ç©ºï¼Œæˆ–è€…æ ¹æ®éœ€æ±‚å»æŸ¥ papers
            # è¿™é‡ŒæŒ‰ç…§éœ€æ±‚ï¼šåªå…³æ³¨å½“æ—¥æ›´æ–°
            
            # ä¸´æ—¶ä¿®æ”¹ get_papers_by_categories æ”¯æŒæŒ‡å®šè¡¨å
            # [Fix] å¢åŠ  limitï¼Œå¦åˆ™é»˜è®¤åªå– 1 æ¡ï¼Œå¦‚æœè¯¥æ¡å·²å¤„ç†åˆ™ä¼šå¯¼è‡´æ— ç»“æœ
            # [Fix] Pass force parameter
            # [Fix] Pass published_date parameter
            papers = self.get_papers_by_categories(expanded_categories, user_id, limit=200, table_name="daily_papers", force=force, published_date=published_date)
            
            
            paper_ids = [p.meta.id for p in papers]
            print(f"Collected Paper IDs for {profile.info.name}: {paper_ids}")
            print(f"User: {profile.info.name}, Pending Paper Count: {len(papers)}")
            
            if not papers:
                print(f"No pending papers found for user {profile.info.name} ({user_id}) in expanded categories {expanded_categories}.")
                from app.schemas.paper import FilterResponse
                from datetime import datetime
                return FilterResponse(
                    user_id=user_id,
                    created_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    total_analyzed=0,
                    accepted_count=0,
                    rejected_count=0,
                    selected_papers=[],
                    rejected_papers=[]
                )

            # 3. æ‰¹é‡ç­›é€‰
            return self.filter_papers(papers, profile, user_id, progress_callback, manual_query, manual_authors, force=force)

        except Exception as e:
            print(f"Error processing pending papers: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›ç©ºç»“æœæˆ–æŠ›å‡ºå¼‚å¸¸
            from app.schemas.paper import FilterResponse
            from datetime import datetime
            return FilterResponse(
                user_id=user_id,
                created_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                total_analyzed=0,
                accepted_count=0,
                rejected_count=0,
                selected_papers=[],
                rejected_papers=[]
            )

    def filter_papers(self, papers: List[PersonalizedPaper], user_profile: UserProfile, user_id: str, progress_callback: Optional[Callable[[int, int, str], None]] = None, manual_query: Optional[str] = None, manual_authors: Optional[List[str]] = None, force: bool = False) -> FilterResponse:
        """
        ä½¿ç”¨ LLM æ‰¹é‡è¿‡æ»¤è®ºæ–‡ (Personalized Filter)ã€‚
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. æ¥æ”¶è®ºæ–‡åˆ—è¡¨å’Œç”¨æˆ·ç”»åƒã€‚
        2. å°†ç”¨æˆ·ç”»åƒçš„å…³é”®éƒ¨åˆ† (Focus, Status) åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²ï¼Œä½œä¸º LLM çš„ Contextã€‚
        3. å¹¶å‘å¤„ç†æ¯ç¯‡è®ºæ–‡ï¼š
            a. æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡ (é¿å…é‡å¤æ¶ˆè€— Token)ã€‚
            b. å°†è®ºæ–‡å…ƒæ•°æ® (Meta) åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²ã€‚
            c. è°ƒç”¨ `_filter_with_retry` å·¥å…·å‡½æ•°ï¼Œä¼ å…¥åºåˆ—åŒ–åçš„ç”»åƒå’Œè®ºæ–‡æ•°æ®ã€‚
        4. è·å– LLM ç»“æœ (Relevance Score, Reason, Accepted)ã€‚
        5. å°†ç»“æœæŒä¹…åŒ–åˆ°æ•°æ®åº“ (`user_paper_states` è¡¨) å¹¶æ›´æ–°å†…å­˜å¯¹è±¡ã€‚
        6. æ„é€ å¹¶è¿”å›åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„ FilterResponseã€‚

        Args:
            papers (List[PersonalizedPaper]): å¾…è¿‡æ»¤çš„è®ºæ–‡åˆ—è¡¨ã€‚
            user_profile (UserProfile): ç”¨æˆ·ç”»åƒå¯¹è±¡ï¼ŒåŒ…å« Focus (å…³æ³¨ç‚¹) å’Œ Status (å½“å‰ä»»åŠ¡/é˜¶æ®µ)ã€‚
            user_id (str): ç”¨æˆ· IDã€‚
            progress_callback (Optional[Callable]): è¿›åº¦å›è°ƒå‡½æ•°ã€‚
            manual_query (Optional[str]): æ‰‹åŠ¨è¾“å…¥çš„è‡ªç„¶è¯­è¨€éœ€æ±‚ (è¦†ç›–ç”¨æˆ·ç”»åƒ)ã€‚
            manual_authors (Optional[List[str]]): æ‰‹åŠ¨è¾“å…¥çš„ä½œè€… (è¦†ç›–ç”¨æˆ·ç”»åƒ)ã€‚

        Returns:
            FilterResponse: åŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œæ‰€æœ‰å¤„ç†è¿‡çš„è®ºæ–‡ç»“æœåˆ—è¡¨ã€‚
        """
        from app.utils.paper_analysis_utils import filter_single_paper

        start_time = time.time()
        
        # --- 1. å‡†å¤‡ç”¨æˆ·ä¸Šä¸‹æ–‡ ---
        # å°†ç”¨æˆ·ç”»åƒä¸­çš„ Focus (å…³æ³¨é¢†åŸŸ/å…³é”®è¯) å’Œ Context (å½“å‰ä»»åŠ¡/åå¥½) æå–å¹¶åºåˆ—åŒ–
        # ç›®çš„ï¼šå‡å°‘ä¼ é€’ç»™ LLM çš„ Token æ•°ï¼Œä»…ä¿ç•™ç­›é€‰å†³ç­–æ‰€éœ€çš„æ ¸å¿ƒä¿¡æ¯
        
        # [Manual Override] å¦‚æœæœ‰æ‰‹åŠ¨è¾“å…¥ï¼Œå®Œå…¨æ›¿æ¢ä¸ºç®€æ´çš„ Markdown æ ¼å¼
        if manual_query or manual_authors:
            print(f"[DEBUG] Using manual query mode - replacing user profile")
            print(f"[DEBUG] Manual query: {manual_query}")
            print(f"[DEBUG] Manual authors: {manual_authors}")
            
            # æ„å»ºç®€æ´çš„ Markdown æ ¼å¼ç”¨æˆ·ç”»åƒ
            profile_parts = []
            
            if manual_query:
                profile_parts.append(f"**å½“å‰æŸ¥è¯¢éœ€æ±‚**ï¼š{manual_query}")
            
            if manual_authors:
                authors_str = "ã€".join(manual_authors)
                profile_parts.append(f"**å…³æ³¨ä½œè€…**ï¼š{authors_str}")
            
            profile_str = "\n\n".join(profile_parts)
            
            print(f"[DEBUG] Manual profile (Markdown):\n{profile_str}")
            
        else:
            # æ­£å¸¸æµç¨‹ï¼šä½¿ç”¨ç”¨æˆ·ç”»åƒ
            # ç§»é™¤ category å­—æ®µï¼Œé¿å…ä¼ å…¥ LLM
            focus_dict = user_profile.focus.model_dump()
            if "category" in focus_dict:
                del focus_dict["category"]
                
            # [Manual Override] å¦‚æœæœ‰æ‰‹åŠ¨è¾“å…¥ï¼Œè¦†ç›– focus ä¸­çš„ keywords/description
            if manual_query:
                print(f"[DEBUG] Applying manual query override: {manual_query}")
                # å‡è®¾ natural_query å¯¹åº” description æˆ– keywords
                # ä¸ºäº†è®© LLM æ˜ç¡®çŸ¥é“è¿™æ˜¯å½“å‰éœ€æ±‚ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥æ›¿æ¢ description
                focus_dict["description"] = manual_query
                # æ¸…ç©º keywords ä»¥é¿å…å¹²æ‰°ï¼Œæˆ–è€…ä¿ç•™ï¼Ÿé€šå¸¸ description æ›´å…·ä½“
                focus_dict["keywords"] = [] 
                
            if manual_authors:
                print(f"[DEBUG] Applying manual authors override: {manual_authors}")
                # å‡è®¾ focus ä¸­æœ‰ authors å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
                focus_dict["authors"] = manual_authors
            
            # æ–°å¢ï¼šæ ¼å¼åŒ– context.preferences åˆ—è¡¨ä¸º Markdown æ ¼å¼
            context_dict = user_profile.context.model_dump()
            context_dict["preferences"] = self._format_preferences_for_llm(
                context_dict.get("preferences", [])
            )

            profile_context = {
                "focus": focus_dict,
                "context": context_dict
            }
            profile_str = json.dumps(profile_context, ensure_ascii=False, indent=2)

        print(f"Filtering {len(papers)} papers with LLM (Concurrent, Max Workers={MAX_WORKERS})...")

        # ç»“æœå®¹å™¨
        selected_papers: List[FilterResultItem] = []
        rejected_papers: List[FilterResultItem] = []
        accepted_count = 0
        rejected_count = 0
        total_retries = 0
        processed_count = 0
        total_papers = len(papers)
        
        # Token ç»Ÿè®¡
        total_tokens_input = 0
        total_tokens_output = 0
        total_cost = 0.0
        total_cache_hit_tokens = 0
        
        # [Batch Update] æ”¶é›†æ‰€æœ‰çŠ¶æ€æ•°æ®
        all_state_data = []

        # å†…éƒ¨é‡è¯•å‡½æ•°
        def _filter_with_retry(paper_str, profile_str):
            retries = 0
            max_retries = 3
            last_error = None
            while retries < max_retries:
                try:
                    result = filter_single_paper(paper_str, profile_str)
                    return result, retries
                except Exception as e:
                    retries += 1
                    last_error = e
                    print(f"Retry {retries}/{max_retries} failed: {e}")
                    time.sleep(1) # ç®€å•çš„é€€é¿
            raise last_error

        # --- 2. å¹¶å‘æ‰§è¡Œ LLM ç­›é€‰ ---
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_paper = {}
            for p in papers:
                # ä¼˜åŒ–ï¼šå¦‚æœå·²ç»æœ‰æ¨èç†ç”±ä¸”ä¸æ˜¯é»˜è®¤å€¼ï¼Œè¯´æ˜å·²å¤„ç†è¿‡
                # [Force] å¦‚æœ force=Trueï¼Œåˆ™å¿½ç•¥å·²å¤„ç†çŠ¶æ€ï¼Œå¼ºåˆ¶é‡æ–°åˆ†æ
                if not force and p.user_state and p.user_state.why_this_paper and p.user_state.why_this_paper != "Not Filtered":
                    # å·²å¤„ç†è¿‡çš„ä¹Ÿè¦åŠ å…¥ç»Ÿè®¡
                    item = FilterResultItem(
                        paper_id=p.meta.id,
                        why_this_paper=p.user_state.why_this_paper,
                        relevance_score=p.user_state.relevance_score,
                        accepted=p.user_state.accepted
                    )
                    if item.accepted:
                        selected_papers.append(item)
                        accepted_count += 1
                    else:
                        rejected_papers.append(item)
                        rejected_count += 1
                    
                    processed_count += 1
                    continue
                
                # å‡†å¤‡è®ºæ–‡æ•°æ®: ä»…ä½¿ç”¨ meta éƒ¨åˆ† (Title, Abstract ç­‰)
                # åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œç¡®ä¿å·¥å…·å‡½æ•°æ¥æ”¶çš„æ˜¯çº¯æ–‡æœ¬ï¼Œé¿å…åœ¨å·¥å…·å‡½æ•°å†…é‡å¤åºåˆ—åŒ–
                paper_dict = p.meta.model_dump()
                paper_str = json.dumps(paper_dict, ensure_ascii=False, indent=2)
                
                # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                future = executor.submit(_filter_with_retry, paper_str, profile_str)
                future_to_paper[future] = p

            # --- 3. å¤„ç†ç­›é€‰ç»“æœ ---
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            total_tasks = len(future_to_paper)
            
            for future in tqdm(as_completed(future_to_paper), total=total_tasks, desc="Filtering Papers"):
                p = future_to_paper[future]
                processed_count += 1
                
                if progress_callback:
                    progress_callback(processed_count, total_tasks, f"æ­£åœ¨ç­›é€‰: {p.meta.title[:30]}...")
                
                
                try:
                    filter_result, retries = future.result()
                    total_retries += retries
                    
                    # ç´¯åŠ  Token æ¶ˆè€—
                    usage = filter_result.get("_usage", {})
                    total_tokens_input += usage.get("prompt_tokens", 0)
                    total_tokens_output += usage.get("completion_tokens", 0)
                    total_cost += usage.get("cost", 0.0)
                    total_cache_hit_tokens += usage.get("cache_hit_tokens", 0)
                    
                    # æ„é€ çŠ¶æ€æ•°æ®å­—å…¸
                    state_data = {
                        "user_id": user_id,
                        "paper_id": p.meta.id,
                        "relevance_score": filter_result["relevance_score"],
                        "why_this_paper": filter_result["why_this_paper"],
                        "accepted": filter_result["accepted"],
                        "user_liked": None,
                        "user_feedback": None,
                        "note": None
                    }

                    # [Batch Update] æ”¶é›†æ•°æ®ï¼Œä¸ç«‹å³å†™å…¥
                    all_state_data.append(state_data)

                    # b. æ›´æ–°å†…å­˜ä¸­çš„è®ºæ–‡å¯¹è±¡çŠ¶æ€ (è™½ç„¶ä¸å†ç›´æ¥è¿”å›åˆ—è¡¨ï¼Œä½†æ›´æ–°å¯¹è±¡æ˜¯ä¸ªå¥½ä¹ æƒ¯)
                    p.user_state = UserPaperState(**state_data)

                    # c. æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                    item = FilterResultItem(
                        paper_id=p.meta.id,
                        why_this_paper=filter_result["why_this_paper"],
                        relevance_score=filter_result["relevance_score"],
                        accepted=filter_result["accepted"]
                    )
                    
                    if item.accepted:
                        selected_papers.append(item)
                        accepted_count += 1
                    else:
                        rejected_papers.append(item)
                        rejected_count += 1

                except Exception as e:
                    print(f"Error filtering paper {p.meta.id} after retries: {e}")
                    # å‘ç”Ÿé”™è¯¯æ—¶ä¸è®¡å…¥ç»Ÿè®¡æˆ–ä½œä¸ºå¤±è´¥å¤„ç†ï¼Œè¿™é‡Œç®€å•è·³è¿‡

        # [Batch Update] æ‰¹é‡å†™å…¥æ•°æ®åº“
        if all_state_data:
            # print(f"Batch updating {len(all_state_data)} user paper states...")
            try:
                # åˆ†æ‰¹å†™å…¥ï¼Œé˜²æ­¢ä¸€æ¬¡æ€§åŒ…è¿‡å¤§
                batch_size = 100
                for i in range(0, len(all_state_data), batch_size):
                    batch = all_state_data[i:i + batch_size]
                    self.db.table("user_paper_states").upsert(batch).execute()
                # print("Batch update user states completed.")
            except Exception as e:
                print(f"Error batch updating user paper states: {e}")

        # --- 4. æ’åºä¸æ„é€ è¿”å›å¯¹è±¡ ---
        # æŒ‰ relevance_score é™åºæ’åˆ—
        selected_papers.sort(key=lambda x: x.relevance_score, reverse=True)
        rejected_papers.sort(key=lambda x: x.relevance_score, reverse=True)

        end_time = time.time()
        total_time = end_time - start_time
        
        # æ±‡æ€»æ—¥å¿—
        print(f"Filtering Completed.")
        print(f"Total Time: {total_time:.2f}s")
        print(f"Total Papers: {total_papers}")
        print(f"Accepted: {accepted_count}, Rejected: {rejected_count}")
        print(f"Total Retries: {total_retries}")
        
        # è®¡ç®—æ€» Token æ¶ˆè€— (ä» all_state_data æˆ–é‡æ–°éå†)
        # ç”±äºæˆ‘ä»¬æ²¡æœ‰åœ¨ all_state_data ä¿å­˜ usageï¼Œæˆ‘ä»¬éœ€è¦åœ¨ future å¤„ç†å¾ªç¯ä¸­ç´¯åŠ 
        # ä¸ºäº†é¿å…å¤§è§„æ¨¡é‡å†™ï¼Œæˆ‘ä»¬å‡è®¾ _filter_with_retry è¿”å›äº† usage (éœ€è¦ä¿®æ”¹ _filter_with_retry)
        # ä½† _filter_with_retry æ˜¯å†…éƒ¨å‡½æ•°ï¼Œä¿®æ”¹å®ƒéœ€è¦ä¿®æ”¹ ThreadPoolExecutor çš„è°ƒç”¨
        # è®©æˆ‘ä»¬ç®€åŒ–ï¼šåœ¨ future.result() ä¸­è·å– usage
        
        # è¿™é‡Œçš„ä»£ç å—æ˜¯ filter_papers çš„å°¾éƒ¨ï¼Œæ— æ³•è®¿é—® executor å¾ªç¯ä¸­çš„å±€éƒ¨å˜é‡
        # å› æ­¤å¿…é¡»é‡å†™ filter_papers çš„ executor å¾ªç¯éƒ¨åˆ†
        
        from datetime import datetime
        return FilterResponse(
            user_id=user_id,
            created_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            total_analyzed=accepted_count + rejected_count,
            accepted_count=accepted_count,
            rejected_count=rejected_count,
            selected_papers=selected_papers,
            rejected_papers=rejected_papers,
            tokens_input=total_tokens_input,
            tokens_output=total_tokens_output,
            cost=total_cost,
            cache_hit_tokens=total_cache_hit_tokens,
            request_count=accepted_count + rejected_count
        )

    def analyze_paper(self, paper: PersonalizedPaper) -> Optional[dict]:
        """
        ä½¿ç”¨ LLM å¯¹å•ç¯‡è®ºæ–‡è¿›è¡Œæ·±åº¦åˆ†æ (Public Analysis)ã€‚
        ä»…å½“è®ºæ–‡å°šæœªåˆ†æ (analysis ä¸ºç©º) æ—¶æ‰§è¡Œã€‚
        
        Args:
            paper (PersonalizedPaper): å¾…åˆ†æçš„è®ºæ–‡å¯¹è±¡ã€‚

        Returns:
            Optional[dict]: éœ€è¦æ›´æ–°åˆ° daily_papers çš„æ•°æ®å­—å…¸ (åŒ…å« id, details, status)ã€‚
                            å¦‚æœåˆ†æå¤±è´¥æˆ–æ— å†…å®¹ï¼Œè¿”å› Noneã€‚
        """
        from app.utils.paper_analysis_utils import analyze_paper_content

        # å‡†å¤‡æ•°æ®: ä»…ä½¿ç”¨ meta
        paper_dict = paper.meta.model_dump()
        
        # è°ƒç”¨å·¥å…·å‡½æ•°è¿›è¡Œåˆ†æ
        analysis_dict = analyze_paper_content(paper_dict)
        
        # [Debug] æ‰“å°åŸå§‹è¿”å›æ•°æ®
        # if analysis_dict:
            # print(f"ğŸ” [DEBUG] Paper {paper.meta.id} LLM Raw Output Keys: {list(analysis_dict.keys())}")
            # é¿å…æ‰“å°è¿‡é•¿ï¼Œåªæ‰“å°å‰ 200 å­—ç¬¦
            # print(f"ğŸ” [DEBUG] Content Preview: {str(analysis_dict)[:200]}...")
        
        # æ£€æŸ¥è¿”å›ç»“æœæ˜¯å¦æœ‰æ•ˆ (å¿…é¡»åŒ…å«å…³é”®å­—æ®µï¼Œä¸”ä¸èƒ½åªæ˜¯ç©ºå­—å…¸æˆ–ä»…æœ‰ _usage)
        if not analysis_dict or (len(analysis_dict) == 1 and "_usage" in analysis_dict):
            print(f"âš ï¸ è®ºæ–‡ {paper.meta.id} åˆ†æç»“æœä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡æ›´æ–°ã€‚")
            return None

        # æå– usageï¼Œé¿å…å­˜å…¥ details
        usage = analysis_dict.pop("_usage", {})
        
        # æ„é€ æ›´æ–°æ•°æ®
        # [Fix] åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µä»¥é¿å… upsert æ—¶çš„éç©ºçº¦æŸé”™è¯¯
        # è™½ç„¶ç†è®ºä¸Š upsert åªæ›´æ–°æŒ‡å®šå­—æ®µï¼Œä½†ä¸ºäº†ç¨³å¦¥ï¼Œæˆ‘ä»¬å¸¦ä¸Šæ‰€æœ‰å…ƒæ•°æ®
        update_data = {
            "id": paper.meta.id,
            "title": paper.meta.title,
            "authors": paper.meta.authors,
            "published_date": str(paper.meta.published_date) if paper.meta.published_date else None,
            "category": paper.meta.category,
            "abstract": paper.meta.abstract,
            # [Fix] åºåˆ—åŒ– links å¯¹è±¡ä¸ºå­—å…¸ï¼Œé¿å… JSON åºåˆ—åŒ–é”™è¯¯
            "links": paper.meta.links.model_dump() if hasattr(paper.meta.links, 'model_dump') else paper.meta.links,
            "comment": paper.meta.comment,
            "details": analysis_dict,
            "status": "analyzed",
            "_usage": usage # é€ä¼  usage ç»™è°ƒç”¨è€…ï¼Œä½†ä¸å­˜å…¥ details å­—æ®µ
        }
            
        return update_data
    def batch_analyze_papers(self, papers: List[PersonalizedPaper], progress_callback: Optional[Callable[[int, int, str], None]] = None) -> Dict[str, Any]:
        """
        å¹¶å‘æ‰¹é‡åˆ†æè®ºæ–‡ (Public Analysis)ã€‚
        [Modified] å¹¶å‘åˆ†æåï¼Œæ‰¹é‡å†™å…¥ daily_papers æ•°æ®åº“ã€‚
        [Modified] æ”¹ä¸ºæ¯ 5 ç¯‡å†™å…¥ä¸€æ¬¡ï¼Œä»¥æä¾›æ›´å¿«çš„åé¦ˆå¹¶é˜²æ­¢æ•°æ®ä¸¢å¤±ã€‚
        [Optimized] ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å¹¶å‘æ•°ï¼Œæ‰¹é‡å®Œæˆåè¾“å‡ºæ±‡æ€»ç»Ÿè®¡ã€‚

        Args:
            papers (List[PersonalizedPaper]): å¾…åˆ†æçš„è®ºæ–‡åˆ—è¡¨ã€‚
            progress_callback (Optional[Callable]): è¿›åº¦å›è°ƒå‡½æ•°ã€‚
        
        Returns:
            Dict[str, int]: Token æ¶ˆè€—ç»Ÿè®¡ {"tokens_input": int, "tokens_output": int}
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from app.core.config import settings
        
        # ç­›é€‰å‡ºæœªåˆ†æçš„è®ºæ–‡
        papers_to_analyze = [p for p in papers if not (p.analysis and p.analysis.motivation)]
        
        if not papers_to_analyze:
            print("No papers need analysis.")
            return {"tokens_input": 0, "tokens_output": 0}

        print(f"Analyzing {len(papers_to_analyze)} papers content (Concurrent)...")
        
        results = []
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å¹¶å‘æ•°
        max_workers = settings.LLM_MAX_WORKERS
        
        # å®šä¹‰å†™å…¥æ‰¹æ¬¡å¤§å°
        write_batch_size = 5
        
        # ç»Ÿè®¡æ•°æ®
        stats = {
            "tokens_input": 0,
            "tokens_output": 0,
            "cost": 0.0,
            "cache_hit_tokens": 0,
            "request_count": 0
        }

        failed_count = 0

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_paper = {executor.submit(self.analyze_paper, p): p for p in papers_to_analyze}
            
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            completed_count = 0
            total_count = len(papers_to_analyze)
            
            for future in tqdm(as_completed(future_to_paper), total=total_count, desc="Analyzing Papers"):
                p = future_to_paper[future]
                completed_count += 1
                if progress_callback:
                    progress_callback(completed_count, total_count, f"æ­£åœ¨åˆ†æ: {p.meta.title[:30]}...")
                try:
                    data = future.result()
                    if data:
                        # æå– usage (analyze_paper å·²ç»å°† _usage æ”¾åœ¨é¡¶å±‚)
                        usage = data.pop("_usage", {})
                        
                        stats["tokens_input"] += usage.get("prompt_tokens", 0)
                        stats["tokens_output"] += usage.get("completion_tokens", 0)
                        stats["cost"] += usage.get("cost", 0.0)
                        stats["cache_hit_tokens"] += usage.get("cache_hit_tokens", 0)
                        stats["request_count"] += 1
                        
                        results.append(data)
                    else:
                        failed_count += 1
                        stats["request_count"] += 1 # å¤±è´¥ä¹Ÿç®—ä¸€æ¬¡è¯·æ±‚å°è¯•
                except Exception as e:
                    print(f"Error analyzing paper {p.meta.id}: {e}")
                    failed_count += 1
                    stats["request_count"] += 1
                
                # æ‰¹é‡å†™å…¥ (æ¯ write_batch_size ç¯‡)
                if len(results) >= write_batch_size:
                    try:
                        self.db.table("daily_papers").upsert(results).execute()
                        # print(f"Saved {len(results)} analyzed papers.")
                        results = []
                    except Exception as e:
                        print(f"Error saving batch analysis: {e}")

            # å†™å…¥å‰©ä½™çš„
            if results:
                try:
                    self.db.table("daily_papers").upsert(results).execute()
                    # print(f"Saved {len(results)} analyzed papers.")
                except Exception as e:
                    print(f"Error saving remaining analysis: {e}")
        
        # [Optimized] æ‰¹é‡å®Œæˆåè¾“å‡ºæ±‡æ€»ç»Ÿè®¡ï¼ˆä½¿ç”¨ finally ç¡®ä¿ä¸€å®šè¾“å‡ºï¼‰
        try:
            success_count = total_count - failed_count
            print(f"âœ… æ‰¹é‡åˆ†æå®Œæˆ: æˆåŠŸ {success_count}/{total_count} ç¯‡ " +
                  f"| Tokens: {stats['tokens_input']}(è¾“å…¥) + {stats['tokens_output']}(è¾“å‡º) = {stats['tokens_input'] + stats['tokens_output']}(æ€»è®¡) " +
                  f"| æˆæœ¬: ${stats['cost']:.4f}")
            
            if failed_count > 0:
                print(f"âš ï¸ Analysis completed with {failed_count} failures.")
        except Exception as e:
            print(f"Error printing summary: {e}")

        return stats

    def get_paper_by_id(self, paper_id: str, user_id: str) -> Optional[PersonalizedPaper]:
        """
        æ ¹æ® ID è·å–å•ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ã€‚

        Args:
            paper_id (str): è®ºæ–‡çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
            user_id (str): ç”¨æˆ· IDï¼Œç”¨äºè·å–è¯¥ç”¨æˆ·å¯¹è¯¥è®ºæ–‡çš„çŠ¶æ€ã€‚

        Returns:
            Optional[PersonalizedPaper]: å¦‚æœæ‰¾åˆ°åˆ™è¿”å›è®ºæ–‡å¯¹è±¡ï¼Œå¦åˆ™è¿”å› Noneã€‚
        """
        try:
            # 1. Fetch Paper
            response = self.db.table("papers").select("*").eq("id", paper_id).execute()
            if not response.data:
                return None
            paper_data = response.data[0]
            
            # 2. Fetch State
            state_data = None
            if user_id:
                state_resp = self.db.table("user_paper_states").select("*").eq("user_id", user_id).eq("paper_id", paper_id).execute()
                if state_resp.data:
                    state_data = state_resp.data[0]
            
            return self.merge_paper_state(paper_data, state_data)
        except Exception as e:
            print(f"Error fetching paper {paper_id}: {e}")
            return None

    def get_papers_by_ids_with_user(self, paper_ids: List[str], user_id: str) -> List[PersonalizedPaper]:
        """
        æ‰¹é‡è·å–è®ºæ–‡è¯¦æƒ…ï¼ŒåŒ…å«ç”¨æˆ·çŠ¶æ€ã€‚

        Args:
            paper_ids (List[str]): è®ºæ–‡ ID åˆ—è¡¨ã€‚
            user_id (str): ç”¨æˆ· IDã€‚

        Returns:
            List[PersonalizedPaper]: è®ºæ–‡å¯¹è±¡åˆ—è¡¨ã€‚
        """
        try:
            if not paper_ids:
                return []

            # 1. æŸ¥è¯¢å…¬å…±è®ºæ–‡åº“
            response = self.db.table("papers").select("*").in_("id", paper_ids).execute()
            papers_data = response.data if response.data else []
            
            # 2. æŸ¥è¯¢ç”¨æˆ·çŠ¶æ€
            states_map = {}
            if user_id:
                state_resp = self.db.table("user_paper_states").select("*").in_("paper_id", paper_ids).eq("user_id", user_id).execute()
                if state_resp.data:
                    for s in state_resp.data:
                        states_map[s['paper_id']] = s
            
            # 3. åˆå¹¶
            results = []
            for p in papers_data:
                state = states_map.get(p['id'])
                results.append(self.merge_paper_state(p, state))
                
            return results
        except Exception as e:
            print(f"æ‰¹é‡è·å–è®ºæ–‡å¤±è´¥: {e}")
            return []

    def get_paper_dates(self, user_id: str, year: int, month: int) -> List[str]:
        """
        è·å–æŒ‡å®šæœˆä»½ä¸­å­˜åœ¨å·²æ¥å—è®ºæ–‡çš„æ—¥æœŸåˆ—è¡¨ã€‚

        Args:
            user_id (str): ç”¨æˆ· IDã€‚
            year (int): å¹´ä»½ã€‚
            month (int): æœˆä»½ã€‚

        Returns:
            List[str]: æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨ (YYYY-MM-DD)ã€‚
        """
        try:
            # æ„å»ºæ—¥æœŸèŒƒå›´
            start_date = f"{year}-{month:02d}-01"
            if month == 12:
                end_date = f"{year + 1}-01-01"
            else:
                end_date = f"{year}-{month + 1:02d}-01"

            # æŸ¥è¯¢ user_paper_states
            # ç­›é€‰æ¡ä»¶: user_id, accepted=True, created_at åœ¨èŒƒå›´å†…
            response = self.db.table("user_paper_states") \
                .select("created_at") \
                .eq("user_id", user_id) \
                .eq("accepted", True) \
                .gte("created_at", start_date) \
                .lt("created_at", end_date) \
                .execute()

            if not response.data:
                return []

            # æå–æ—¥æœŸå¹¶å»é‡
            dates = set()
            for item in response.data:
                # created_at æ˜¯ ISO æ ¼å¼ (e.g., 2023-11-29T10:00:00+00:00)
                # æˆªå–å‰ 10 ä½ä½œä¸ºæ—¥æœŸ
                date_str = item["created_at"][:10]
                dates.add(date_str)

            return sorted(list(dates))
        except Exception as e:
            print(f"Error fetching paper dates: {e}")
            return []

    def get_recommendations(self, user_id: str, date: Optional[str] = None) -> List[PersonalizedPaper]:
        """
        è·å–ç”¨æˆ·çš„æ¨èè®ºæ–‡åˆ—è¡¨ (å³å·²è¢«æ ‡è®°ä¸º accepted=True çš„è®ºæ–‡)ã€‚
        æ”¯æŒæŒ‰æ—¥æœŸç­›é€‰ã€‚

        Args:
            user_id (str): ç”¨æˆ· IDã€‚
            date (Optional[str]): ç­›é€‰æ—¥æœŸ (YYYY-MM-DD)ã€‚å¦‚æœä¸º Noneï¼Œåˆ™è¿”å›æ‰€æœ‰ã€‚

        Returns:
            List[PersonalizedPaper]: æ¨èçš„è®ºæ–‡åˆ—è¡¨ã€‚
        """
        try:
            # 1. Get states where why_this_paper is not 'Not Filtered' (meaning it has been analyzed)
            # Previously filtered by accepted=True, now returning all analyzed papers for frontend filtering
            query = self.db.table("user_paper_states").select("*").eq("user_id", user_id).neq("why_this_paper", "Not Filtered")
            
            if date:
                # ç­›é€‰æŒ‡å®šæ—¥æœŸçš„è®°å½• (created_at >= date AND created_at < date + 1 day)
                # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ date æ˜¯ YYYY-MM-DD æ ¼å¼
                next_day = (datetime.strptime(date, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")
                query = query.gte("created_at", date).lt("created_at", next_day)
            
            state_resp = query.execute()
            
            if not state_resp.data:
                return []
            
            states = state_resp.data
            paper_ids = [s['paper_id'] for s in states]
            
            # 2. Get papers
            # Supabase 'in' filter: .in_("id", paper_ids)
            if not paper_ids:
                return []
                
            paper_resp = self.db.table("papers").select("*").in_("id", paper_ids).execute()
            papers_map = {p['id']: p for p in paper_resp.data}
            
            # 3. Merge
            results = []
            for s in states:
                p_data = papers_map.get(s['paper_id'])
                if p_data:
                    results.append(self.merge_paper_state(p_data, s))
            return results
        except Exception as e:
            print(f"Error fetching recommendations: {e}")
            return []

    def archive_daily_papers(self) -> bool:
        """
        å°† daily_papers è¡¨ä¸­çš„æ•°æ®å½’æ¡£åˆ° papers è¡¨ã€‚
        [Fix] ä½¿ç”¨æ‰¹å¤„ç†ä»¥é¿å… Errno 35 èµ„æºè€—å°½é”™è¯¯ã€‚
        """
        print("Starting archiving daily papers to public DB...")
        try:
            # 1. è·å–æ‰€æœ‰ daily_papers
            # å¦‚æœæ•°æ®é‡éå¸¸å¤§ï¼Œè¿™é‡Œä¹Ÿåº”è¯¥åˆ†é¡µè·å–ï¼Œä½†é€šå¸¸ daily_papers ä¸ä¼šå¤ªå¤§ (å‡ ç™¾æ¡)
            response = self.db.table("daily_papers").select("*").execute()
            daily_papers = response.data
            
            if not daily_papers:
                print("No daily papers to archive.")
                return True
                
            print(f"Found {len(daily_papers)} papers to archive.")
            
            # 2. æ‰¹é‡æ’å…¥åˆ° papers è¡¨
            # ä½¿ç”¨ upsert (on_conflict="id") é¿å…é‡å¤
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ 50 æ¡
            batch_size = 50
            total = len(daily_papers)
            
            for i in range(0, total, batch_size):
                batch = daily_papers[i:i + batch_size]
                try:
                    self.db.table("papers").upsert(batch).execute()
                    # print(f"Archived batch {i//batch_size + 1}/{(total + batch_size - 1)//batch_size}")
                    # å°ç¡ä¸€ä¸‹ï¼Œé‡Šæ”¾èµ„æº
                    time.sleep(0.1)
                except Exception as e:
                    print(f"Error archiving batch {i}: {e}")
                    # å³ä½¿æŸæ‰¹å¤±è´¥ï¼Œä¹Ÿå°è¯•ç»§ç»­ä¸‹ä¸€æ‰¹ï¼Ÿæˆ–è€…ç›´æ¥å¤±è´¥ï¼Ÿ
                    # è¿™é‡Œé€‰æ‹©è®°å½•é”™è¯¯ä½†ç»§ç»­ï¼Œå°½å¯èƒ½å¤šåœ°å½’æ¡£
            
            print("Daily papers archived successfully.")
            return True
            
        except Exception as e:
            print(f"Error archiving papers: {e}")
            return False

paper_service = PaperService()