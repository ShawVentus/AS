"""
run_crawler_step.py
å·¥ä½œæµæ­¥éª¤ï¼šè¿è¡Œçˆ¬è™«ã€‚

è´Ÿè´£è°ƒç”¨ Scrapy çˆ¬è™«æŠ“å–æŒ‡å®šç±»åˆ«çš„è®ºæ–‡ã€‚
"""

from typing import Dict, Any
from app.core.workflow_step import WorkflowStep
from app.services.workflow_service import workflow_service

class RunCrawlerStep(WorkflowStep):
    """
    æ­¥éª¤ï¼šè¿è¡Œçˆ¬è™«ã€‚
    """
    name = "run_crawler"
    max_retries = 3 # çˆ¬è™«å®¹æ˜“å—ç½‘ç»œå½±å“ï¼Œå…è®¸é‡è¯•
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œçˆ¬è™«é€»è¾‘ï¼ŒæŠ“å–æŒ‡å®šç±»åˆ«çš„ ArXiv è®ºæ–‡ã€‚
        
        åŠŸèƒ½è¯´æ˜ï¼š
        1. ä» context æˆ– system_status è·å– ArXiv æ—¥æœŸ
        2. æ£€æŸ¥å“ªäº›ç±»åˆ«å·²ç»çˆ¬å–è¿‡
        3. è¿è¡Œ Scrapy çˆ¬è™«æŠ“å–ç¼ºå¤±çš„ç±»åˆ«
        4. æ›´æ–° system_status è®°å½•å·²çˆ¬å–çš„ç±»åˆ«
        5. ä»æ•°æ®åº“æŸ¥è¯¢ä»Šæ—¥çˆ¬å–çš„è®ºæ–‡æ•°é‡ï¼ˆä½¿ç”¨ created_at å­—æ®µï¼‰
        
        Args:
            context (Dict[str, Any]): å·¥ä½œæµä¸Šä¸‹æ–‡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ
                - categories (List[str]): è¦çˆ¬å–çš„ç±»åˆ«åˆ—è¡¨ï¼ˆå¿…éœ€ï¼‰
                - force (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°çˆ¬å–ï¼ˆå¯é€‰ï¼Œé»˜è®¤ Falseï¼‰
                - arxiv_date (str): ArXiv æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        
        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µ
                - crawler_run (bool): æ˜¯å¦æ‰§è¡Œäº†çˆ¬è™«
                - crawled_categories (List[str]): æœ¬æ¬¡çˆ¬å–çš„ç±»åˆ«
                - crawled_count (int): ä»Šæ—¥çˆ¬å–çš„è®ºæ–‡æ•°é‡
                - total_found_count (int): åŒ crawled_count
                - skipped (bool): æ˜¯å¦è·³è¿‡ï¼ˆå¯é€‰ï¼‰
        
        Raises:
            Exception: çˆ¬è™«æ‰§è¡Œå¤±è´¥æ—¶æŠ›å‡º
        """
        categories = context.get("categories")
        force = context.get("force", False)
        
        if not categories:
            self.update_progress(100, 100, "æ²¡æœ‰æŒ‡å®šåˆ†ç±»ï¼Œè·³è¿‡çˆ¬å–")
            return {"crawler_run": False}
            
        # 1. è·å–å½“å‰ Arxiv æ—¥æœŸ
        # ä¼˜å…ˆä» context è·å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™å‡è®¾æ˜¯ä»Šå¤© (æˆ–è€…ç”± scheduler æ³¨å…¥)
        # æ³¨æ„: å¦‚æœ context ä¸­æ²¡æœ‰ dateï¼Œæˆ‘ä»¬å¯èƒ½æ— æ³•å‡†ç¡®åˆ¤æ–­ "ä»Šæ—¥å·²çˆ¬å–"
        # ä½†é€šå¸¸ scheduler.run_daily_workflow ä¼šå…ˆ check_arxiv_update å¹¶æ”¾å…¥ context
        from datetime import datetime
        import json
        from app.core.database import get_db
        
        db = get_db()
        
        # å°è¯•è·å– Arxiv æ—¥æœŸï¼Œå¦‚æœ context ä¸­æ²¡æœ‰ï¼Œåˆ™å°è¯•ä» system_status è·å– last_arxiv_update
        arxiv_date = context.get("arxiv_date")
        if not arxiv_date:
            status_row = db.table("system_status").select("*").eq("key", "last_arxiv_update").execute()
            if status_row.data:
                arxiv_date = status_row.data[0]["value"]
            else:
                # Fallback to today
                arxiv_date = datetime.now().strftime("%A, %d %B %Y") # Arxiv format approx
        
        # 2. æ£€æŸ¥ system_status ä¸­çš„ daily_crawl_status
        # æ ¼å¼: {"date": "...", "categories": ["cs.CL"]}
        status_key = "daily_crawl_status"
        crawl_status_row = db.table("system_status").select("*").eq("key", status_key).execute()
        
        existing_categories = set()
        if crawl_status_row.data:
            try:
                status_data = crawl_status_row.data[0]
                # value å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ– JSON å¯¹è±¡ï¼Œå–å†³äºå­˜å‚¨æ–¹å¼ã€‚Supabase client é€šå¸¸è¿”å›è§£æåçš„ JSON
                val = status_data["value"]
                if isinstance(val, str):
                    val = json.loads(val)
                
                if val.get("date") == arxiv_date:
                    existing_categories = set(val.get("categories", []))
                
                print(f"[DEBUG] RunCrawlerStep: arxiv_date={arxiv_date}, status_date={val.get('date')}, existing={existing_categories}")
            except Exception as e:
                print(f"è§£æ daily_crawl_status å¤±è´¥: {e}")
        else:
            print(f"[DEBUG] RunCrawlerStep: No daily_crawl_status found for key {status_key}")
        
        # 3. è®¡ç®—éœ€è¦çˆ¬å–çš„åˆ†ç±»
        target_categories = set(categories)
        missing_categories = list(target_categories - existing_categories)
        
        if force:
            print(f"[DEBUG] RunCrawlerStep: Force mode enabled. Target: {target_categories}, Existing: {existing_categories}, Missing: {missing_categories}")
        
        if not missing_categories:
            if force:
                print(f"[DEBUG] RunCrawlerStep: Force mode enabled. Re-crawling all target categories: {target_categories}")
                missing_categories = list(target_categories)
            else:
                # [ä¿®å¤] è·³è¿‡çˆ¬è™«æ—¶ï¼Œæœ¬æ¬¡çˆ¬å–æ•°é‡åº”ä¸º 0
                # åŸå› ï¼šæ‰€æœ‰ç±»åˆ«å·²çˆ¬å–ï¼Œæœ¬æ¬¡æ‰§è¡Œæ²¡æœ‰çˆ¬å–ä»»ä½•æ–°è®ºæ–‡
                # ä¹‹å‰çš„æŸ¥è¯¢ä¼šå¾—åˆ°ä»Šå¤©æ‰€æœ‰è®ºæ–‡æ•°ï¼ˆé”™è¯¯ï¼ï¼‰
                print(f"[INFO] æ‰€æœ‰åˆ†ç±» ({', '.join(categories)}) å·²çˆ¬å–ï¼Œè·³è¿‡çˆ¬è™«")
                self.update_progress(100, 100, f"æ‰€æœ‰åˆ†ç±»å·²çˆ¬å–ï¼Œè·³è¿‡")
                
                return {
                    "crawler_run": False, 
                    "skipped": True,
                    "crawled_count": 0,  # æœ¬æ¬¡æœªçˆ¬å–ä»»ä½•è®ºæ–‡
                    "total_found_count": 0
                }
        
        
        # 4. è¿è¡Œçˆ¬è™«
        try:
            # [ä¿®æ”¹] æ¥æ”¶çˆ¬è™«è¿”å›çš„ç»Ÿè®¡æ•°æ®
            crawler_stats = workflow_service.run_crawler(missing_categories)
            
            # [ä¿®å¤] ç›´æ¥ä½¿ç”¨çˆ¬è™«è¿”å›çš„çœŸå®æ•°é‡ï¼Œç§»é™¤ fallback é€»è¾‘
            # åŸå› è¯´æ˜ï¼š
            # 1. å¦‚æœçˆ¬è™«è§£æå¤±è´¥ï¼Œrun_crawler ä¼šæŠ›å‡ºå¼‚å¸¸ï¼ˆåœ¨ workflow_service å±‚å¤„ç†ï¼‰
            # 2. å¦‚æœçˆ¬è™«çœŸçš„çˆ¬å–0ç¯‡ï¼Œyielded=0 æ˜¯æ­£å¸¸æƒ…å†µï¼ˆç±»åˆ«æ— æ–°è®ºæ–‡ï¼‰
            # 3. ä¹‹å‰çš„ fallback æŸ¥è¯¢æ•°æ®åº“ä¼šå¾—åˆ°ä»Šå¤©æ‰€æœ‰è®ºæ–‡æ•°ï¼ˆé”™è¯¯çš„æ€»æ•°ï¼‰
            crawled_count = crawler_stats.get("yielded", 0)
            
            print(f"[INFO] çˆ¬è™«ç»Ÿè®¡: æœ¬æ¬¡æäº¤å¤„ç† {crawled_count} ç¯‡è®ºæ–‡")
            
            # 5. æ›´æ–° system_status (åœ¨è¿è¡Œçˆ¬è™«åæ›´æ–°ï¼Œè¡¨ç¤ºè¿™äº›åˆ†ç±»å·²çˆ¬å–)
            new_categories = list(existing_categories.union(set(missing_categories)))
            new_status = {
                "date": arxiv_date,
                "categories": new_categories,
                "updated_at": datetime.now().isoformat()
            }
            
            # Upsert
            db.table("system_status").upsert({
                "key": status_key,
                "value": new_status
            }).execute()

            # 6. è¿”å›ç»“æœï¼ˆä½¿ç”¨çˆ¬è™«ç»Ÿè®¡çš„çœŸå®æ•°é‡ï¼‰
            # [è°ƒè¯•] è¾“å‡ºçˆ¬è™«ç»Ÿè®¡è¯¦æƒ…ï¼Œç”¨äºéªŒè¯æ•°æ®æµ
            print(f"[DEBUG] run_crawler_step è¿”å›å€¼è¯¦æƒ…:")
            print(f"  - crawled_count (æäº¤å¤„ç†): {crawled_count}")
            print(f"  - unique_found (å»é‡å): {crawler_stats.get('unique_found', 'N/A')}")
            print(f"  - total_found (åŸå§‹æŠ“å–): {crawler_stats.get('total_found', 'N/A')}")
            print(f"  - skipped_category (åˆ†ç±»ä¸ç¬¦): {crawler_stats.get('skipped_category', 'N/A')}")
            
            # [ä¼˜åŒ–] æ„å»ºè¯¦ç»†çš„çˆ¬è™«ç»Ÿè®¡æ¶ˆæ¯ï¼Œä¼ é€’ç»™å‰ç«¯æ˜¾ç¤º
            unique_found = crawler_stats.get("unique_found", crawled_count)
            total_found = crawler_stats.get("total_found", crawled_count)
            skipped = crawler_stats.get("skipped_category", 0)
            
            stats_msg = (
                f"ğŸ“„ å‘ç° {unique_found} ç¯‡è®ºæ–‡ (åŸå§‹ {total_found}) | "
                f"âœ… æäº¤ {crawled_count} ç¯‡ | "
                f"ğŸš« è·³è¿‡ {skipped} ç¯‡"
            )
            
            print(f"[INFO] çˆ¬è™«ç»Ÿè®¡: {stats_msg}")
            self.update_progress(100, 100, stats_msg)
            
            return {
                "crawler_run": True, 
                "crawled_categories": missing_categories, 
                "crawled_count": crawled_count,  # ä½¿ç”¨çˆ¬è™«è¿”å›çš„çœŸå®æ•°é‡
                "total_found_count": crawled_count
            }
            
        except Exception as e:
            # [ä¿®å¤] æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
            error_msg = f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {str(e)}"
            print(f"[ERROR] {error_msg}")
            self.update_progress(0, 100, error_msg)
            raise e
