import os
import json
from typing import Dict, Any, Optional
from openai import OpenAI
from app.core.config import settings

class QwenService:
    def __init__(self):
        """
        åˆå§‹åŒ– QwenService æœåŠ¡ã€‚
        
        åŠŸèƒ½ï¼š
            ä»é…ç½®ä¸­åŠ è½½ LLM è®¾ç½®ï¼Œåˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ã€‚
            æ”¯æŒåŠ¨æ€åˆ‡æ¢ API æº (OpenRouter, DashScope, Bohrium)ã€‚
        """
        try:
            config = settings.get_llm_config()
            self.client = OpenAI(
                api_key=config["api_key"],
                base_url=config["base_url"]
            )
            self.model = config["model"]
            self.provider = settings.LLM_PROVIDER
            print(f"âœ“ LLM æœåŠ¡åˆå§‹åŒ–æˆåŠŸ | æº: {self.provider} | æ¨¡å‹: {self.model}")
        except Exception as e:
            print(f"âœ— LLM æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
            self.model = ""

    def read_prompt(self, filename: str) -> str:
        """
        ä»æ–‡ä»¶ç³»ç»Ÿä¸­è¯»å–æç¤ºè¯æ¨¡æ¿ã€‚

        Args:
            filename (str): æç¤ºè¯æ¨¡æ¿çš„æ–‡ä»¶å (ä¾‹å¦‚ "filter.md")ã€‚

        Returns:
            str: æç¤ºè¯æ¨¡æ¿çš„å†…å®¹ã€‚
        """
        # backend/services/llm_service.py -> backend/prompt
        path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "prompt", filename)
        with open(path, "r", encoding="utf-8") as f:
            return f.read()

    def call_llm(self, prompt: str, model: str = None, response_format: Optional[Dict[str, Any]] = {"type": "json_object"}) -> tuple[str, Dict[str, Any]]:
        """
        è°ƒç”¨ LLM API æ‰§è¡Œè¯·æ±‚ (åŒ…å«é‡è¯•æœºåˆ¶)ã€‚
        æ”¯æŒåŠ¨æ€åˆ‡æ¢æ¨¡å‹ï¼Œå¹¶è§£æ OpenRouter çš„æˆæœ¬ä¸ç¼“å­˜ä¿¡æ¯ã€‚

        Args:
            prompt (str): å‘é€ç»™ LLM çš„å®Œæ•´æç¤ºè¯å­—ç¬¦ä¸²ã€‚
            model (str, optional): æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ã€‚å¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤é…ç½®çš„æ¨¡å‹ã€‚
            response_format (Dict[str, Any], optional): API è¿”å›æ ¼å¼é…ç½®ã€‚é»˜è®¤ä¸º {"type": "json_object"}ã€‚

        Returns:
            tuple[str, Dict[str, Any]]: (å†…å®¹å­—ç¬¦ä¸², ä½¿ç”¨ç»Ÿè®¡å­—å…¸)ã€‚
                                        ç»Ÿè®¡å­—å…¸åŒ…å«:
                                        - 'prompt_tokens': è¾“å…¥ Token æ•°
                                        - 'completion_tokens': è¾“å‡º Token æ•°
                                        - 'total_tokens': æ€» Token æ•°
                                        - 'cost': é¢„ä¼°æˆ–å®é™…æˆæœ¬ (USD)
                                        - 'cache_hit_tokens': ç¼“å­˜å‘½ä¸­çš„ Token æ•°
                                        - 'model': å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
                                        å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å› ("{}", {})ã€‚
        """
        import time
        import traceback
        
        if not self.client:
            print("âœ— LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œè¯·æ±‚")
            return "{}", {}

        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        target_model = model if model else self.model
        
        max_retries = 3
        base_delay = 2
        
        for attempt in range(max_retries + 1):
            try:
                completion = self.client.chat.completions.create(
                    model=target_model,
                    messages=[
                        {"role": "system", "content": self.read_prompt("system.md")},
                        {"role": "user", "content": prompt},
                    ],
                    temperature=1.2,
                    response_format=response_format # ä½¿ç”¨ä¼ å…¥çš„æ ¼å¼é…ç½®
                )
                response = completion.choices[0].message.content
                
                # è§£æ Usage å’Œ Cost
                usage = completion.usage
                usage_dict = {
                    "prompt_tokens": usage.prompt_tokens if usage else 0,
                    "completion_tokens": usage.completion_tokens if usage else 0,
                    "total_tokens": usage.total_tokens if usage else 0,
                    "model": target_model
                }

                # å°è¯•è§£æ OpenRouter ç‰¹æœ‰å­—æ®µ (cost, cache)
                # æ³¨æ„: OpenAI Python å®¢æˆ·ç«¯å¯èƒ½å°†é¢å¤–å­—æ®µæ”¾åœ¨ model_extra æˆ–ç›´æ¥å±æ€§ä¸­
                # è¿™é‡Œçš„å¤„ç†å°è¯•å…¼å®¹ä¸åŒçš„è¿”å›ç»“æ„
                
                # 1. å°è¯•è·å–ç¼“å­˜å‘½ä¸­æ•°
                # OpenRouter é€šå¸¸åœ¨ prompt_tokens_details ä¸­è¿”å› cached_tokens
                if usage and hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details:
                     # æ£€æŸ¥æ˜¯å¦ä¸ºå¯¹è±¡æˆ–å­—å…¸
                    details = usage.prompt_tokens_details
                    if isinstance(details, dict):
                        usage_dict["cache_hit_tokens"] = details.get("cached_tokens", 0)
                    elif hasattr(details, 'cached_tokens'):
                        usage_dict["cache_hit_tokens"] = details.cached_tokens
                    else:
                        usage_dict["cache_hit_tokens"] = 0
                else:
                    usage_dict["cache_hit_tokens"] = 0

                # 2. å°è¯•è·å– Cost (OpenRouter ç‰¹æœ‰)
                # æŸäº›å®¢æˆ·ç«¯ç‰ˆæœ¬å¯èƒ½å°†éæ ‡å‡†å­—æ®µæ”¾åœ¨ extra_fields æˆ– model_extra
                # å¦‚æœæ— æ³•ç›´æ¥è·å–ï¼Œåç»­ WorkflowEngine ä¼šæ ¹æ® Token è®¡ç®—
                if hasattr(completion, 'usage') and isinstance(completion.usage, dict):
                     usage_dict["cost"] = completion.usage.get("cost", 0.0)
                elif hasattr(completion, 'model_extra') and completion.model_extra:
                     usage_info = completion.model_extra.get('usage', {})
                     if isinstance(usage_info, dict):
                         usage_dict["cost"] = usage_info.get('cost', 0.0)
                
                # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
                response = response.strip()
                if response.startswith("```"):
                    lines = response.split('\n')
                    if lines[0].startswith("```"):
                        lines = lines[1:]
                    if lines and lines[-1].strip() == "```":
                        lines = lines[:-1]
                    response = '\n'.join(lines)
                
                return response.strip(), usage_dict
                
            except Exception as e:
                error_str = str(e)
                import traceback
                traceback_str = traceback.format_exc()
                
                if "429" in error_str or "Rate limit" in error_str:
                    if attempt < max_retries:
                        delay = base_delay * (2 ** attempt)
                        print(f"âš ï¸ LLM é€Ÿç‡é™åˆ¶ (429), {delay}ç§’åé‡è¯•... ({attempt + 1}/{max_retries})")
                        time.sleep(delay)
                        continue
                
                print(f"âŒ LLM è°ƒç”¨é”™è¯¯: {e}")
                print(f"ğŸ” é”™è¯¯å †æ ˆ: {traceback_str}")
                # é 429 é”™è¯¯æˆ–é‡è¯•è€—å°½ï¼Œè¿”å›ç©º
                return "{}", {}
                
        return "{}", {}

    def filter_paper(self, paper: Dict, user_profile: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æ£€æŸ¥è®ºæ–‡æ˜¯å¦ä¸ç”¨æˆ·ç”»åƒç›¸å…³ã€‚

        Args:
            paper (Dict): è®ºæ–‡å…ƒæ•°æ®å­—å…¸ã€‚
            user_profile (str): ç”¨æˆ·ç”»åƒå­—ç¬¦ä¸²ã€‚

        Returns:
            Dict[str, Any]: ç­›é€‰ç»“æœå­—å…¸ (åŒ…å« _usage)ã€‚
        """
        template = self.read_prompt("filter.md")
        prompt = template.format(
            user_profile=user_profile,
            title=paper.get("title", ""),
            abstract=paper.get("abstract", ""),
            category=paper.get("category", "")
        )
        
        # ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹è¿›è¡Œç­›é€‰
        response, usage = self.call_llm(prompt, model=settings.OPENROUTER_MODEL_CHEAP)
        try:
            result = json.loads(response)
            result["_usage"] = usage # æ³¨å…¥ usage ä¿¡æ¯
            return result
        except json.JSONDecodeError:
            print(f"filter è§£æ JSON é”™è¯¯: {response}")
            return {"is_relevant": False, "score": 0, "reason": "Parse Error", "_usage": usage}

    def analyze_paper(self, abstract: str, comment: str = "") -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM åˆ†æè®ºæ–‡è¯¦æƒ…ã€‚

        Args:
            abstract (str): è®ºæ–‡æ‘˜è¦ã€‚
            comment (str): è®ºæ–‡å¤‡æ³¨ (å¯é€‰)ã€‚

        Returns:
            Dict[str, Any]: åˆ†æç»“æœå­—å…¸ (åŒ…å« _usage)ã€‚
        """
        template = self.read_prompt("analyze.md")
        
        prompt = template.format(
            abstract=abstract,
            comment=comment
        )
        
        # ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹è¿›è¡Œåˆ†æ
        response, usage = self.call_llm(prompt, model=settings.OPENROUTER_MODEL_CHEAP)
        try:
            result = json.loads(response)
            result["_usage"] = usage
            return result
        except json.JSONDecodeError:
            print(f"analyze è§£æ JSON é”™è¯¯: {response}")
            return {"_usage": usage}

    def generate_report(self, papers: list, user_profile: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆæ¯æ—¥æŠ¥å‘Šã€‚

        Args:
            papers (list): è®ºæ–‡åˆ—è¡¨ã€‚
            user_profile (str): ç”¨æˆ·ç”»åƒå­—ç¬¦ä¸²ã€‚

        Returns:
            Dict[str, Any]: æŠ¥å‘Šç”Ÿæˆç»“æœ (åŒ…å« _usage)ã€‚
        """
        template = self.read_prompt("report.md")
        
        # ä¸ºæç¤ºè¯æ ¼å¼åŒ–è®ºæ–‡åˆ—è¡¨
        papers_text = ""
        for p in papers:
            papers_text += f"ID: {p['id']}\nTitle: {p['title']}\nAbstract: {p['abstract'][:200]}...\n\n"
            
        # ä½¿ç”¨ replace æ›¿ä»£ formatï¼Œé¿å… JSON å¤§æ‹¬å·å†²çª
        from datetime import datetime
        import time
        import re
        
        current_date = datetime.now().strftime("%Y/%m/%d")
        prompt = template.replace("{user_profile}", user_profile).replace("{papers}", papers_text).replace("{date}", current_date)
        
        # å®šä¹‰è§£æè¾…åŠ©å‡½æ•°
        def _parse_report_response(response_text: str) -> Optional[Dict[str, Any]]:
            """
            è§£æ LLM è¿”å›çš„ XML æ ‡ç­¾æ ¼å¼æŠ¥å‘Šã€‚
            
            Args:
                response_text (str): LLM è¿”å›çš„åŸå§‹æ–‡æœ¬ã€‚
                
            Returns:
                Optional[Dict[str, Any]]: è§£æåçš„å­—å…¸ï¼ŒåŒ…å« title, summary, contentã€‚è§£æå¤±è´¥è¿”å› Noneã€‚
            """
            try:
                # ä½¿ç”¨éè´ªå©ªåŒ¹é…æå–æ ‡ç­¾å†…å®¹ï¼Œre.DOTALL å…è®¸åŒ¹é…æ¢è¡Œç¬¦
                title_match = re.search(r'<title>(.*?)</title>', response_text, re.DOTALL)
                summary_match = re.search(r'<summary>(.*?)</summary>', response_text, re.DOTALL)
                content_match = re.search(r'<content>(.*?)</content>', response_text, re.DOTALL)
                
                if not (title_match and summary_match and content_match):
                    print(f"âŒ Report parsing failed. Missing tags. Response preview: {response_text[:200]}...")
                    return None
                
                return {
                    "title": title_match.group(1).strip(),
                    "summary": summary_match.group(1).strip(),
                    "content": content_match.group(1).strip()
                }
            except Exception as e:
                print(f"âŒ Report parsing exception: {e}")
                return None

        # å®šä¹‰é‡è¯•é€»è¾‘
        def try_generate(model_name, retries=3):
            """
            å°è¯•ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚
            
            Args:
                model_name (str): æ¨¡å‹åç§°ã€‚
                retries (int): é‡è¯•æ¬¡æ•°ã€‚
                
            Returns:
                Optional[Dict[str, Any]]: ç”Ÿæˆå¹¶è§£æåçš„ç»“æœå­—å…¸ã€‚
            """
            for i in range(retries):
                print(f"Generating report with {model_name} (Attempt {i+1}/{retries})...")
                # ç§»é™¤ response_format={"type": "json_object"}ï¼Œå…è®¸è‡ªç”±æ ¼å¼è¾“å‡º
                # æ˜¾å¼ä¼ å…¥ None ä»¥è¦†ç›–é»˜è®¤çš„ JSON æ¨¡å¼
                response, usage = self.call_llm(prompt, model=model_name, response_format=None)
                
                if usage and response and response != "{}":
                    parsed_result = _parse_report_response(response)
                    if parsed_result:
                        parsed_result["_usage"] = usage
                        return parsed_result
                
                if i < retries - 1:
                    time.sleep(2) # é‡è¯•é—´éš”
            return None

        # 1. å°è¯•ä¸»æ¨¡å‹
        result = try_generate(settings.OPENROUTER_MODEL_PERFORMANCE, retries=3)
        
        # 2. å¦‚æœä¸»æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹
        if not result:
            print(f"âš ï¸ Primary model {settings.OPENROUTER_MODEL_PERFORMANCE} failed after 3 attempts. Switching to fallback...")
            fallback_model = "deepseek/deepseek-v3.2"
            result = try_generate(fallback_model, retries=3)
            
        if result:
            # è‡ªåŠ¨å¡«å…… ref_papers (ç›´æ¥ä½¿ç”¨è¾“å…¥çš„è®ºæ–‡åˆ—è¡¨ ID)
            result["ref_papers"] = [p['id'] for p in papers]
            # å†æ¬¡ç¡®è®¤ fallback æ¨¡å‹æˆåŠŸæ—¥å¿—
            if result.get("_usage", {}).get("model") == "deepseek/deepseek-v3.2":
                 print(f"âœ“ Report generated successfully with fallback model deepseek/deepseek-v3.2")
            return result
            
        print("âŒ All report generation attempts failed.")
        return {"_usage": {}} # è¿”å›ç©º usage è¡¨ç¤ºå½»åº•å¤±è´¥

    def extract_categories(self, text: str) -> Dict[str, Any]:
        """
        ä»è‡ªç„¶è¯­è¨€ä¸­æå– Arxiv ç±»åˆ«å’Œä½œè€…ã€‚

        Args:
            text (str): ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚

        Returns:
            Dict[str, Any]: åŒ…å« categories å’Œ authors çš„å­—å…¸ (åŒ…å« _usage)ã€‚
                            - categories: æå–å‡ºçš„ Arxiv ç±»åˆ«åˆ—è¡¨ã€‚
                            - authors: æå–å‡ºçš„ä½œè€…åˆ—è¡¨ã€‚
        """
        template = self.read_prompt("extract_categories.md")
        prompt = template.replace("{user_input}", text)
        
        # ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹è¿›è¡Œæå–
        response, usage = self.call_llm(prompt, model=settings.OPENROUTER_MODEL_CHEAP)
        try:
            result = json.loads(response)
            result["_usage"] = usage
            return result
        except json.JSONDecodeError:
            print(f"extract_categories è§£æ JSON é”™è¯¯: {response}")
            # Fallback: å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„å…³é”®è¯åŒ¹é…æˆ–è¿”å›ç©º
            return {"categories": [], "authors": [], "_usage": usage}

llm_service = QwenService()
