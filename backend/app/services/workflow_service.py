import os
import subprocess
import sys
from typing import List, Optional, Callable, Dict, Any
from app.core.database import get_db
from app.services.paper_service import paper_service
from app.schemas.paper import PersonalizedPaper, RawPaperMetadata
from crawler.fetch_details import fetch_and_update_details

class WorkflowService:
    def __init__(self):
        self.db = get_db()

    def get_active_execution(self, user_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        è·å–å½“å‰æ´»è·ƒçš„æ‰§è¡Œè®°å½• (running or pending)ã€‚
        """
        import json
        try:
            # æŸ¥æ‰¾çŠ¶æ€ä¸º running æˆ– pending çš„è®°å½•
            response = self.db.table("workflow_executions") \
                .select("*") \
                .in_("status", ["running", "pending"]) \
                .order("created_at", desc=True) \
                .execute()
            
            executions = response.data
            if not executions:
                return None
                
            for exec_record in executions:
                # æ£€æŸ¥ metadata ä¸­çš„ target_user_id
                metadata_str = exec_record.get("metadata", "{}")
                # Handle case where metadata might be dict or string
                if isinstance(metadata_str, dict):
                    metadata = metadata_str
                else:
                    metadata = json.loads(metadata_str)
                
                target_user_id = metadata.get("target_user_id")
                
                # å¦‚æœæŒ‡å®šäº† user_idï¼Œå¿…é¡»åŒ¹é…
                if user_id:
                    if target_user_id == user_id:
                        return exec_record
                else:
                    # å¦‚æœæ²¡æŒ‡å®š user_idï¼Œè¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„
                    return exec_record
                    
            return None
        except Exception as e:
            print(f"Error getting active execution: {e}")
            return None

    def run_crawler(self, categories: Optional[List[str]] = None):
        """
        è¿è¡Œ ArXiv çˆ¬è™«ä»»åŠ¡ã€‚
        
        é€šè¿‡ subprocess è°ƒç”¨ Scrapy çˆ¬è™«ï¼ŒæŠ“å–æœ€æ–°çš„è®ºæ–‡æ•°æ®å¹¶å­˜å…¥æ•°æ®åº“ã€‚
        æ”¯æŒä¼ å…¥ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚æœä¼ å…¥åˆ™åªçˆ¬å–æŒ‡å®šç±»åˆ«ã€‚

        Args:
            categories (Optional[List[str]]): éœ€è¦çˆ¬å–çš„ç±»åˆ«åˆ—è¡¨ã€‚å¦‚æœä¸ä¼ ï¼Œçˆ¬è™«å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤é…ç½®ã€‚

        Returns:
            None
        """
        print("Starting ArXiv Crawler...")
        try:
            # cwd should be backend root (where scrapy.cfg is)
            backend_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
            print(f"Running Scrapy in: {backend_root}")
            
            cmd = ["scrapy", "crawl", "arxiv"]
            
            # å¦‚æœæœ‰ç±»åˆ«å‚æ•°ï¼Œé€šè¿‡ -a ä¼ é€’ç»™ spider
            if categories:
                categories_str = ",".join(categories)
                print(f"Crawling specific categories: {categories_str}")
                cmd.extend(["-a", f"categories={categories_str}"])
            
            subprocess.run(cmd, check=True, cwd=backend_root)
            print("Crawler finished.")
            
        except Exception as e:
            print(f"Crawler failed: {e}")
            raise e

    def analyze_public_papers(self, progress_callback: Optional[Callable[[int, int, str], None]] = None):
        """
        å¤„ç†å…¬å…±è®ºæ–‡åˆ†æã€‚
        
        è·å–çŠ¶æ€ä¸º 'fetched' çš„æ–°è®ºæ–‡ï¼Œå¹¶è¿›è¡Œå…¬å…±åˆ†æï¼ˆå¦‚ç”Ÿæˆ TLDRã€æå– Motivation ç­‰ï¼‰ã€‚
        è¿™äº›åˆ†æç»“æœæ˜¯é€šç”¨çš„ï¼Œä¸é’ˆå¯¹ç‰¹å®šç”¨æˆ·ã€‚
        
        [Modified] åˆ†æ‰¹å¤„ç†ï¼šæ¯æ‰¹ 20 ç¯‡ï¼Œæ‰¹æ¬¡é—´ç­‰å¾… 60 ç§’ã€‚

        Args:
            progress_callback (Optional[Callable]): è¿›åº¦å›è°ƒã€‚

        Returns:
            None
        """
        import time
        print("Starting Public Analysis...")
        try:
            print("--- Public Analysis ---")
            # è·å–å°šæœªåˆ†æçš„è®ºæ–‡ (status ä¸º fetched)
            response = self.db.table("daily_papers").select("*").eq("status", "fetched").execute()
            raw_papers = response.data
            
            papers_to_analyze = []
            for p in raw_papers:
                # æ„é€  PersonalizedPaper (analysis=None, user_state=None)
                meta_data = {
                    "id": p["id"],
                    "title": p["title"],
                    "authors": p["authors"],
                    "published_date": p["published_date"],
                    "category": p["category"],
                    "abstract": p["abstract"],
                    "links": p["links"],
                    "comment": p.get("comment")
                }
                meta = RawPaperMetadata(**meta_data)
                papers_to_analyze.append(PersonalizedPaper(meta=meta, analysis=None, user_state=None))
            
            total_papers = len(papers_to_analyze)
            
            # ç»Ÿè®¡æ•°æ®
            total_stats = {
                "tokens_input": 0,
                "tokens_output": 0,
                "cost": 0.0,
                "cache_hit_tokens": 0,
                "request_count": 0
            }
            
            if total_papers > 0:
                print(f"Found {total_papers} papers needing public analysis.")
                
                # åˆ†æ‰¹å¤„ç†
                batch_size = 20
                delay_seconds = 60
                
                for i in range(0, total_papers, batch_size):
                    batch = papers_to_analyze[i:i + batch_size]
                    print(f"Processing batch {i // batch_size + 1}/{(total_papers + batch_size - 1) // batch_size} (Size: {len(batch)})...")
                    
                    # ä¼ é€’ progress_callback
                    # æ³¨æ„ï¼šbatch_analyze_papers å†…éƒ¨æ˜¯é’ˆå¯¹ batch çš„å¾ªç¯
                    # å¦‚æœæˆ‘ä»¬å¸Œæœ›è¿›åº¦æ˜¯å…¨å±€çš„ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œåšä¸€ä¸ªé€‚é…å™¨ï¼Œæˆ–è€…è®© batch_analyze_papers åªå¤„ç†å±€éƒ¨è¿›åº¦
                    # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è®© batch_analyze_papers å¤„ç†å±€éƒ¨è¿›åº¦ï¼Œä½†è¿™é‡Œæˆ‘ä»¬æ— æ³•è½»æ˜“åˆå¹¶
                    # æ›´å¥½çš„æ–¹å¼æ˜¯ï¼šåœ¨ analyze_public_papers è¿™é‡Œæ§åˆ¶æ€»è¿›åº¦
                    
                    # å®šä¹‰å±€éƒ¨å›è°ƒé€‚é…å™¨
                    def batch_callback(current, total, msg):
                        if progress_callback:
                            # è®¡ç®—å…¨å±€è¿›åº¦
                            # i æ˜¯å½“å‰æ‰¹æ¬¡çš„èµ·å§‹ç´¢å¼•
                            global_current = i + current
                            global_total = total_papers
                            progress_callback(global_current, global_total, msg)

                    batch_stats = paper_service.batch_analyze_papers(batch, progress_callback=batch_callback)
                    
                    # ç´¯åŠ ç»Ÿè®¡
                    if batch_stats:
                        total_stats["tokens_input"] += batch_stats.get("tokens_input", 0)
                        total_stats["tokens_output"] += batch_stats.get("tokens_output", 0)
                        total_stats["cost"] += batch_stats.get("cost", 0.0)
                        total_stats["cache_hit_tokens"] += batch_stats.get("cache_hit_tokens", 0)
                        total_stats["request_count"] += batch_stats.get("request_count", 0)
                    
                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œç­‰å¾…
                    if i + batch_size < total_papers:
                        print(f"Waiting {delay_seconds} seconds before next batch...")
                        time.sleep(delay_seconds)
            else:
                print("No papers need public analysis.")
                
            # [Modified] Add analyzed_count to stats
            total_stats["analyzed_count"] = total_papers
                
            return total_stats

        except Exception as e:
            print(f"Error in analyze_public_papers: {e}")
            raise e

    def process_public_papers_workflow(self, categories: Optional[List[str]] = None):
        """
        æ‰§è¡Œå…¬å…±è®ºæ–‡å¤„ç†å·¥ä½œæµã€‚
        
        æµç¨‹ï¼š
        1. è¿è¡Œçˆ¬è™« (run_crawler)
        2. è·å–è¯¦æƒ… (fetch_and_update_details)
        3. å…¬å…±åˆ†æ (analyze_public_papers)
        4. å½’æ¡£ (archive_daily_papers)

        Args:
            categories (Optional[List[str]]): éœ€è¦çˆ¬å–çš„ç±»åˆ«åˆ—è¡¨ã€‚

        Returns:
            None
        """
        print("ğŸš€ Starting Public Papers Workflow...")
        
        try:
            # 1. Run Crawler
            print("\nğŸ•·ï¸  Step 1: Running Crawler...")
            self.run_crawler(categories)
            
            # 2. Fetch Details
            print("\nğŸ“¥ Step 2: Fetching Details from Arxiv API...")
            fetch_and_update_details(table_name="daily_papers")
            
            # 3. Analyze
            print("\nğŸ§  Step 3: Running Public Analysis...")
            self.analyze_public_papers()
            
            # 4. Archive
            print("\nğŸ’¾ Step 4: Archiving to Public DB...")
            if paper_service.archive_daily_papers():
                print("âœ… Archiving completed.")
            else:
                print("âŒ Archiving failed.")
                
            print("ğŸ‰ Public Papers Workflow Completed!")
            
        except Exception as e:
            print(f"âŒ Public Papers Workflow Failed: {e}")
            # Re-raise to let caller know it failed
            raise e

workflow_service = WorkflowService()
