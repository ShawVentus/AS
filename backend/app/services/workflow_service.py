import os
import subprocess
import sys
from typing import List, Optional, Callable, Dict, Any
from app.core.database import get_db
from app.services.paper_service import paper_service
from app.schemas.paper import PersonalizedPaper, RawPaperMetadata
from crawler.fetch_details import fetch_and_update_details

class WorkflowService:
    def __init__(self):
        self.db = get_db()

    def get_active_execution(self, user_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        è·å–å½“å‰æ´»è·ƒçš„æ‰§è¡Œè®°å½• (running or pending)ã€‚
        """
        import json
        try:
            # æŸ¥æ‰¾çŠ¶æ€ä¸º running æˆ– pending çš„è®°å½•
            response = self.db.table("workflow_executions") \
                .select("*") \
                .in_("status", ["running", "pending"]) \
                .order("created_at", desc=True) \
                .execute()
            
            executions = response.data
            if not executions:
                return None
                
            for exec_record in executions:
                # æ£€æŸ¥ metadata ä¸­çš„ target_user_id
                metadata_str = exec_record.get("metadata", "{}")
                # Handle case where metadata might be dict or string
                if isinstance(metadata_str, dict):
                    metadata = metadata_str
                else:
                    metadata = json.loads(metadata_str)
                
                target_user_id = metadata.get("target_user_id")
                
                # å¦‚æœæŒ‡å®šäº† user_idï¼Œå¿…é¡»åŒ¹é…
                if user_id:
                    if target_user_id == user_id:
                        return exec_record
                else:
                    # å¦‚æœæ²¡æŒ‡å®š user_idï¼Œè¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„
                    return exec_record
                    
            return None
        except Exception as e:
            print(f"Error getting active execution: {e}")
            return None

    def run_crawler(self, categories: Optional[List[str]] = None) -> Dict[str, int]:
        """
        è¿è¡Œ ArXiv çˆ¬è™«ä»»åŠ¡ã€‚
        
        é€šè¿‡ subprocess è°ƒç”¨ Scrapy çˆ¬è™«ï¼ŒæŠ“å–æœ€æ–°çš„è®ºæ–‡æ•°æ®å¹¶å­˜å…¥æ•°æ®åº“ã€‚
        æ”¯æŒä¼ å…¥ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚æœä¼ å…¥åˆ™åªçˆ¬å–æŒ‡å®šç±»åˆ«ã€‚
        
        [ä¿®æ”¹] ç°åœ¨ä¼šæ•è·çˆ¬è™«è¾“å‡ºå¹¶è§£æç»Ÿè®¡æ•°æ®ï¼Œè¿”å›çœŸå®çš„çˆ¬å–æ•°é‡ã€‚

        Args:
            categories (Optional[List[str]]): éœ€è¦çˆ¬å–çš„ç±»åˆ«åˆ—è¡¨ã€‚å¦‚æœä¸ä¼ ï¼Œçˆ¬è™«å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤é…ç½®ã€‚

        Returns:
            Dict[str, int]: çˆ¬è™«ç»Ÿè®¡æ•°æ®ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
                - yielded (int): æäº¤å¤„ç†çš„è®ºæ–‡æ•°é‡ï¼ˆå»é‡åçš„çœŸå®çˆ¬å–æ•°ï¼‰
                - unique_found (int): å‘ç°çš„å”¯ä¸€è®ºæ–‡æ•°
                - total_found (int): æ€»å…±å‘ç°çš„è®ºæ–‡æ•°ï¼ˆå«é‡å¤ï¼‰
        """
        print("Starting ArXiv Crawler...")
        try:
            # cwd should be backend root (where scrapy.cfg is)
            backend_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
            print(f"Running Scrapy in: {backend_root}")
            
            cmd = ["scrapy", "crawl", "arxiv"]
            
            # å¦‚æœæœ‰ç±»åˆ«å‚æ•°ï¼Œé€šè¿‡ -a ä¼ é€’ç»™ spider
            if categories:
                categories_str = ",".join(categories)
                print(f"Crawling specific categories: {categories_str}")
                cmd.extend(["-a", f"categories={categories_str}"])
            
            # [ä¿®æ”¹] æ•è·çˆ¬è™«è¾“å‡ºï¼Œä»¥ä¾¿è§£æç»Ÿè®¡æ•°æ®
            result = subprocess.run(cmd, check=True, cwd=backend_root, 
                                   capture_output=True, text=True)
            
            # [æ–°å¢] è§£æçˆ¬è™«è¾“å‡ºçš„ JSON_STATS
            # çˆ¬è™«ä¼šåœ¨è¾“å‡ºçš„æœ€åä¸€è¡Œæ‰“å°ç»Ÿè®¡æ•°æ®: JSON_STATS:{...}
            import json
            
            stats = None  # åˆå§‹ä¸º Noneï¼Œè¡¨ç¤ºå°šæœªæˆåŠŸè§£æ
            
            # ä» stdout ä¸­æå– JSON_STATS è¡Œ
            for line in result.stdout.split('\n'):
                if line.startswith("JSON_STATS:"):
                    json_str = line.replace("JSON_STATS:", "")
                    try:
                        stats = json.loads(json_str)
                        print(f"[DEBUG] æˆåŠŸè§£æçˆ¬è™«ç»Ÿè®¡: yielded={stats.get('yielded', 0)}, unique_found={stats.get('unique_found', 0)}")
                        break  # æ‰¾åˆ°å°±é€€å‡ºå¾ªç¯ï¼Œä¸å†è§£æåç»­è¡Œ
                    except Exception as e:
                        print(f"[ERROR] JSON_STATS è§£æå¤±è´¥: {e}")
                        print(f"[ERROR] åŸå§‹æ•°æ®: {json_str}")
            
            # [ä¿®å¤] å¦‚æœè§£æå¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            # åŸå› ï¼šè§£æå¤±è´¥è¯´æ˜çˆ¬è™«æ‰§è¡Œæœ‰é—®é¢˜ï¼Œä¸åº”è¯¥ç»§ç»­æ‰§è¡Œå·¥ä½œæµ
            if stats is None:
                print("[ERROR] çˆ¬è™«æœªè¾“å‡ºæœ‰æ•ˆçš„ JSON_STATS")
                print("[ERROR] çˆ¬è™«å¯èƒ½å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥çˆ¬è™«ä»£ç ")
                # åªæ‰“å°æœ€å20è¡Œç”¨äºè°ƒè¯•ï¼ˆé¿å…æ—¥å¿—è¿‡å¤šï¼‰
                lines = result.stdout.split('\n')
                print("[DEBUG] çˆ¬è™«è¾“å‡ºçš„æœ€å20è¡Œ:")
                for line in lines[-20:]:
                    print(f"  {line}")
                raise Exception("çˆ¬è™«ç»Ÿè®¡æ•°æ®è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥çˆ¬è™«è¾“å‡º")
            
            print("Crawler finished.")
            return stats  # è¿”å›ç»Ÿè®¡æ•°æ®
            
        except Exception as e:
            print(f"Crawler failed: {e}")
            raise e

    def analyze_public_papers(self, progress_callback: Optional[Callable[[int, int, str], None]] = None):
        """
        å¤„ç†å…¬å…±è®ºæ–‡åˆ†æã€‚
        
        è·å–çŠ¶æ€ä¸º 'fetched' çš„æ–°è®ºæ–‡ï¼Œå¹¶è¿›è¡Œå…¬å…±åˆ†æï¼ˆå¦‚ç”Ÿæˆ TLDRã€æå– Motivation ç­‰ï¼‰ã€‚
        è¿™äº›åˆ†æç»“æœæ˜¯é€šç”¨çš„ï¼Œä¸é’ˆå¯¹ç‰¹å®šç”¨æˆ·ã€‚
        
        [Modified] åˆ†æ‰¹å¤„ç†ï¼šæ¯æ‰¹ 20 ç¯‡ï¼Œæ‰¹æ¬¡é—´ç­‰å¾… 60 ç§’ã€‚

        Args:
            progress_callback (Optional[Callable]): è¿›åº¦å›è°ƒã€‚

        Returns:
            None
        """
        import time
        print("Starting Public Analysis...")
        try:
            print("--- Public Analysis ---")
            # è·å–å°šæœªåˆ†æçš„è®ºæ–‡ (status ä¸º fetched)
            response = self.db.table("daily_papers").select("*").eq("status", "fetched").execute()
            raw_papers = response.data
            
            papers_to_analyze = []
            for p in raw_papers:
                # æ„é€  PersonalizedPaper (analysis=None, user_state=None)
                meta_data = {
                    "id": p["id"],
                    "title": p["title"],
                    "authors": p["authors"],
                    "published_date": p["published_date"],
                    "category": p["category"],
                    "abstract": p["abstract"],
                    "links": p["links"],
                    "comment": p.get("comment")
                }
                meta = RawPaperMetadata(**meta_data)
                papers_to_analyze.append(PersonalizedPaper(meta=meta, analysis=None, user_state=None))
            
            total_papers = len(papers_to_analyze)
            
            # ç»Ÿè®¡æ•°æ®
            total_stats = {
                "tokens_input": 0,
                "tokens_output": 0,
                "cost": 0.0,
                "cache_hit_tokens": 0,
                "request_count": 0
            }
            
            if total_papers > 0:
                print(f"Found {total_papers} papers needing public analysis.")
                
                # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
                from app.core.config import settings
                batch_size = settings.LLM_ANALYSIS_BATCH_SIZE
                delay_seconds = settings.LLM_ANALYSIS_BATCH_DELAY
                
                for i in range(0, total_papers, batch_size):
                    batch = papers_to_analyze[i:i + batch_size]
                    batch_num = i // batch_size + 1
                    total_batches = (total_papers + batch_size - 1) // batch_size
                    print(f"Processing batch {batch_num}/{total_batches} (Size: {len(batch)})...")
                    
                    # ä¼ é€’ progress_callback
                    # æ³¨æ„ï¼šbatch_analyze_papers å†…éƒ¨æ˜¯é’ˆå¯¹ batch çš„å¾ªç¯
                    # å¦‚æœæˆ‘ä»¬å¸Œæœ›è¿›åº¦æ˜¯å…¨å±€çš„ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œåšä¸€ä¸ªé€‚é…å™¨ï¼Œæˆ–è€…è®© batch_analyze_papers åªå¤„ç†å±€éƒ¨è¿›åº¦
                    # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è®© batch_analyze_papers å¤„ç†å±€éƒ¨è¿›åº¦ï¼Œä½†è¿™é‡Œæˆ‘ä»¬æ— æ³•è½»æ˜“åˆå¹¶
                    # æ›´å¥½çš„æ–¹å¼æ˜¯ï¼šåœ¨ analyze_public_papers è¿™é‡Œæ§åˆ¶æ€»è¿›åº¦
                    
                    # å®šä¹‰å±€éƒ¨å›è°ƒé€‚é…å™¨
                    def batch_callback(current, total, msg):
                        if progress_callback:
                            # è®¡ç®—å…¨å±€è¿›åº¦
                            # i æ˜¯å½“å‰æ‰¹æ¬¡çš„èµ·å§‹ç´¢å¼•
                            global_current = i + current
                            global_total = total_papers
                            progress_callback(global_current, global_total, msg)

                    batch_stats = paper_service.batch_analyze_papers(batch, progress_callback=batch_callback)
                    
                    # ç´¯åŠ ç»Ÿè®¡
                    if batch_stats:
                        total_stats["tokens_input"] += batch_stats.get("tokens_input", 0)
                        total_stats["tokens_output"] += batch_stats.get("tokens_output", 0)
                        total_stats["cost"] += batch_stats.get("cost", 0.0)
                        total_stats["cache_hit_tokens"] += batch_stats.get("cache_hit_tokens", 0)
                        total_stats["request_count"] += batch_stats.get("request_count", 0)
                    
                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œç­‰å¾…
                    if i + batch_size < total_papers:
                        print(f"Waiting {delay_seconds} seconds before next batch...")
                        time.sleep(delay_seconds)
            else:
                print("No papers need public analysis.")
                
            # [Modified] Add analyzed_count to stats
            total_stats["analyzed_count"] = total_papers
            
            # [Optimized] è¾“å‡ºæ‰€æœ‰æ‰¹æ¬¡çš„æ€»æ±‡æ€»
            if total_papers > 0:
                print(f"\n{'='*60}")
                print(f"ğŸ“Š æ€»ä½“åˆ†ææ±‡æ€»:")
                print(f"  - æ€»è®ºæ–‡æ•°: {total_papers} ç¯‡")
                print(f"  - æ€» Tokens: {total_stats['tokens_input']}(è¾“å…¥) + {total_stats['tokens_output']}(è¾“å‡º) = {total_stats['tokens_input'] + total_stats['tokens_output']}(æ€»è®¡)")
                print(f"  - æ€»æˆæœ¬: ${total_stats['cost']:.4f}")
                print(f"  - ç¼“å­˜å‘½ä¸­: {total_stats['cache_hit_tokens']} tokens")
                print(f"  - API è¯·æ±‚æ•°: {total_stats['request_count']} æ¬¡")
                print(f"{'='*60}\n")
                
            return total_stats

        except Exception as e:
            print(f"Error in analyze_public_papers: {e}")
            raise e

    def process_public_papers_workflow(self, categories: Optional[List[str]] = None):
        """
        æ‰§è¡Œå…¬å…±è®ºæ–‡å¤„ç†å·¥ä½œæµã€‚
        
        æµç¨‹ï¼š
        1. è¿è¡Œçˆ¬è™« (run_crawler)
        2. è·å–è¯¦æƒ… (fetch_and_update_details)
        3. å…¬å…±åˆ†æ (analyze_public_papers)
        4. å½’æ¡£ (archive_daily_papers)

        Args:
            categories (Optional[List[str]]): éœ€è¦çˆ¬å–çš„ç±»åˆ«åˆ—è¡¨ã€‚

        Returns:
            None
        """
        print("ğŸš€ Starting Public Papers Workflow...")
        
        try:
            # 1. Run Crawler
            print("\nğŸ•·ï¸  Step 1: Running Crawler...")
            self.run_crawler(categories)
            
            # 2. Fetch Details
            print("\nğŸ“¥ Step 2: Fetching Details from Arxiv API...")
            fetch_and_update_details(table_name="daily_papers")
            
            # 3. Analyze
            print("\nğŸ§  Step 3: Running Public Analysis...")
            self.analyze_public_papers()
            
            # 4. Archive
            print("\nğŸ’¾ Step 4: Archiving to Public DB...")
            if paper_service.archive_daily_papers():
                print("âœ… Archiving completed.")
            else:
                print("âŒ Archiving failed.")
                
            print("ğŸ‰ Public Papers Workflow Completed!")
            
        except Exception as e:
            print(f"âŒ Public Papers Workflow Failed: {e}")
            # Re-raise to let caller know it failed
            raise e

workflow_service = WorkflowService()
