## 开发记录

### 2025-12-18 16:55:27

**上线准备**：

- 用户初始化过程优化 - 增加 ai 生成类别的按钮，提示会直接保存这个需求，并且这个需求可保存到设置中，可不止一条（列表），初始化填写的名字不会储存到信息中，直接去掉初始化，让用户直接生成报告就行，给到引导界面，提示：
  1. 报告生成组件 - 立即体验
  2. 填写信息、ai 润色
  3. 保存 - 作为报告生成根据
- 扣费逻辑 - 次卡 - 2025-12-19 16:01:58
  1. 手动生成研报需要扣费
  2. 自动生成识别
  3. 可选是否自动生成报告
- 类别选择优化 - 2025-12-20 19:23:52
  1. 确认没有错误的检索类别 - OK
  2. 主类别和次类别对应 - OK，后续获取再 debug，先就这样
  3. 测试全类别手动 + 自动化流程
  4. ai 自动生成加上上下文 - OK
  5. 类别前端加上提示 - OK
- 注册与用户管理向玻尔适配
  1. 获取 cookie 的 ak - 先做一个简单的 app 看能否获取 ak，获取之后做传递给前端获取用户信息 - 问 ai 现在的逻辑是什么，给一个方案出来
  2. 官方代码部署应用尝试获取 ak
  3. ak 作为 uuid 初始化用户/作为访问凭证 - 改 supabase 唯一 id
  4. 尝试扣费
  5. 写一个扣费函数
  6. 充值界面
- 部署 app.py - 尽量简单
- 改自动时间
- 有效用户逻辑：增加必须邮件字段不为空
- 部署到 github 上的自动化任务

  1. pythonanywhere/github/定时打开玻尔的机器？

- 上线后：

1. api 优化 ，为什么会重复调用那么多次，逻辑 debug（不重要，可以后面慢慢优化
2. 论文数量少的类别默认全选，而不需要选择子类别。 - 支持全选
3. 首页显示：已为您生成个性化论文/报告 XX 篇，今天是为您服务的第 xx 天
4. 后端意外关闭后，重启前端就会遗留没有更新的报告生成任务
5. 增加提示显示时间
6. 过滤实时提示论文
7. 用户初次注册时没有论文和报告，应该默认放置预览报告，没有研报时也应该提示暂无研报

### 2025-12-17 19:29:13 - 2025-12-18 16:55:17

TODO: 初始化引导 + debug + 引导文字

- 准备上线前：
  1. 修改分析 prompt，避免同质化的语言 - 又修改了一些
  2. 增加错误发送邮箱 - OK 2025-12-18 15:36:07
  3. 重构用户记忆 - OK 2025-12-18 15:33:04
  4. 测试不同邮件显示效果 - 除了 gmail 显示都还可以，但是有一次手动发送邮件失败了
  5. 测试多用户报告生成功能 - OK
  6. 测试手动报告的论文覆盖功能 - OK
  7. 测试最大速率，根据限流情况更改 api 调用速率 - 已经修改到 .env 中
  8. 修复 bug： query 和偏好不应该一起输入给模型，现在只保留 query - OK
  9. 前端实时进度显示优化 - OK
  10. 手动报告结果显示 bug，失败仍会跳转 - OK
  11. 手动报告切换页面信息丢失 - OK
  12. 邮件 debug
  - 示例 论文 + 研报 - 已经有免费的了，用户当然会自己生成，不用专门做示例

### 2025-12-16 07:55:53 - 2025-12-16 23:26:04

1. 邮件页面优化，显示获取论文，推荐论文数
2. 优化报告生成界面，手动生成进度优化 ⭐️
3. 增大报告生成弹窗提示，延长显示时间 -> 现在会直接跳转到已经生成的报告
4. 删除历史研报中生成研报功能
5. api 改为 玻尔官方的
6. 设置增加退出账号按钮（后续应该还是会改，毕竟玻尔的用户就是靠验证登录的，换账号只能从外部换，内部不用改，也改不了）

### 2025-12-15 17:49:29 - 2025-12-15 22:46:49

1. 增加报告和论文详情页动态效果与模糊效果
2. 修复 bug：手动生成报告后，后端已经停止，前端因为没有指令而保持原样（待测试）
3. 增加时间显示，优化时间格式
4. 修复 bug：手动生成报告没有正常写入新论文，没有正常强制覆盖论文的评论
5. 修改 arxiv 逻辑，获取 replacement 以外的论文

### 2025-12-15 10:36:38 - 2025-12-15 17:49:32

1. 将进度和用户填写的信息放到额外的单独页面一起显示，现在是弹窗，且关闭后就没了，固定在单一页面内 - 2025-12-15 12:34:44
2. 增加用户笔记功能
3. 第一次使用的用户在周末生成报告的话，显示的不应该是当日的日期，而是论文实际挂在 arxiv 上的日期
4. 增加论文页面回到顶部按钮，保持顶栏不移动
5. 发现 arxiv 数量对不上的问题 - 逻辑没问题，显示上的问题，爬虫获取需要写入的论文数并未去重，而真正在数据库中的论文是去重过的
6. 修复数据重载、任务进度丢失、全局通知的问题，预加载论文和报告页面的数据，并存入缓存
7. 优化报告显示页面，修复缩进和句号多余空格问题，更换摘要 ui
8. 优化预览/详情卡片显示
9. 论文页面能显示：显示论文/所有论文：35/120
10. 修复头像显示 bug

### 2025-12-14 14:14:55

已经实现前后端状态的实时交互

### 2025-12-12 15:38:17 - 2025-12-12 22:33:52

1. 前后端交互优化，实时显示报告生成进度 - 尝试通过数据库转发状态，但是前端没有办法接收后端的状态
2. 修改 arxiv 逻辑，如果重复就不访问对应的页面爬取论文
3. 论文管理的筛选功能，支持类别多选，并且显示的类别是根据相关度筛选后的结果

### 2025-12-11 23:00:18 - 2025-12-12 15:37:55

测试前后端其他功能，确保没有显示 bug

- 报告生成 + 个性化生成 提示词 debug
  - 实现 \_parse_report_response(response_text) 辅助函数。
  - 使用正则表达式 (re) 提取 <title>, <summary>, <content> 标签内的内容。
  - 优化 filter 提示词
- 报告生成 增加 deepseek 回退 - 增加报警功能 - 修正计数错误
  优化日期显示：前端显示的日期是论文创建日期，创建时全部填入为当日的字段
  计算成本
  修改定时时间为 9:30，冬令时北京时间早上九点更新，预留半小时防止服务器过载

### 2025-12-12 17:48:26 - 2025-12-11 22:59:28

支持成本控制，完善日志记录与输出，优化 pipeline
debug 前后端的部分功能
明确调用成本

### 2025-12-11 10:53:20

完善 html 模板，实现邮件发送功能

### 2025-12-09 22:24:54

完善邮件发送功能，形成 pipeline，待测试功能完整性，待增加用户引导语平台适配设置

### 2025-12-08 20:56:48

优化论文显示页面，优化交互体验

### 2025-12-08 10:33:29

完善 report prompt
预填入报告，准备前端渲染实现
实现个性化论文生成 pipeline

### 2025-12-06 20:33:27

实现前端论文显示界面

### 2025-12-05 22:37:15

scheduler 中实现分析论文与更新用户私有数据库论文的功能
新建 daily_papers 表，用于存放每日更新的论文

### 2025-12-04 15:39:50

实现输入用户 id 即可更新用户私有数据库论文：paper_service.get_papers_by_categories

### 2025-11-29 20:28:44

完全移除模拟数据
完善初始化流程与用户画像初始化
完善 TODO

### 2025-11-29 14:34:38

完善公共论文分析功能，根据 status 判断论文状态，将 completed 状态的论文分析后更新到数据库中设置状态为 analyzed

### 2025-11-29 13:25:50

优化前端体验
初步完善用户初始化界面
优化论文获取逻辑，增加 calendar 功能
优化个人信息设置界面，增加头像上传功能
增加加载的等待界面，首次进入应用时显示 LoadingScreen

### 2025-11-28 17:38:25

添加邮箱注册与登录功能，后续将会实现私有数据库管理

- **用户管理**：注册后没有对应的个人信息，注册时应当搜集用户名等信息。注册后，前端右上角会显示对应的初始头像，点击头像可以进入设置界面。而初始化头像为用户用户名首字母大写/中文，像谷歌一样给到随机颜色的背景。此外，初始化时需要让用户自行初始化需求（将用于生成每日论文推荐，您后续可以随时更改您的需求）
- **个性化用户画像管理**：作者机构支持手动添加，支持改名字头像。支持通过自然语言更新画像（调用简单的免费小型 LLM），更新最近的需求
- **LLM 更新**：调用 LLM 会更新公共论文数据库中的 details 和特定用户私有数据库中的论文，并维护更新对应的字段
- **前端论文显示与管理**：在用户的私有论文库构建完成后，检验前端显示情况，能否显示当日论文与过去论文（时间以在私有数据库中创建为准），并且能够实现链接访问等功能。批量下载后面再说。需要实现 like 字段的存储（like 的意思是以后多生成对应的论文）
- **自动化研报**：
- **自动化邮件**：

### 2025-11-28 15:25:39

优化论文获取与存储到数据库的功能

```bash
# 一次性完成两个阶段
conda activate arxivscout
cd backend

# Stage 1: 抓取 ID 和分类 (约 20-30 秒)
scrapy crawl arxiv

# Stage 2: 获取详细信息 (根据论文数量，可能需要几分钟)
python -m crawler.fetch_details
```

### 2025-11-23 15:45:50

调整代码结构，删除冗余部分

### 2025-11-23 11:03:43

完善前端和爬虫，准备逐步完善其他功能

# Arxiv Scout Agent (Arxiv 侦察兵)

Arxiv Scout Agent 是一个智能化的论文查询与阅读助手，旨在通过个性化记忆模块，为您提供精准的论文筛选、深度分析、研报生成及邮件推送服务。

## 核心特性

- **个性化记忆**: 基于您的阅读历史和反馈（Like/Dislike），不断进化推荐算法。
- **智能分析**: 利用 Qwen LLM 深度解读论文，提取 TLDR、创新点和关键结论。
- **自动化研报**: 每日自动聚合最新论文，生成综述型研报并发送邮件。
- **交互式前端**: 现代化的 React 界面，支持深色模式、交互式引用和可视化记忆仪表盘。

## 技术栈

- **Frontend**: React, Vite, TailwindCSS, Lucide Icons
- **Backend**: FastAPI, Python 3.11
- **Database**: PostgreSQL (via Supabase)
- **LLM**: Qwen (DashScope API)
- **Crawler**: Scrapy
- **Deployment**: Docker Compose

## 快速开始 (Docker 部署)

### 1. 前置准备

- 安装 [Docker](https://www.docker.com/) 和 [Docker Compose](https://docs.docker.com/compose/install/)。
- 注册 [Supabase](https://supabase.com/) 账号并创建一个新项目。
- 获取 [DashScope (通义千问)](https://dashscope.aliyun.com/) API Key。
- 准备一个支持 SMTP 的邮箱（如 QQ 邮箱）用于发送研报。

### 2. 数据库初始化

1. 登录 Supabase Dashboard，进入项目的 **SQL Editor**。
2. 复制本项目 `backend/schema.sql` 文件的全部内容。
3. 在 SQL Editor 中执行该脚本，创建必要的数据表和安全策略。

### 3. 环境变量配置

在项目根目录下创建或修改 `.env` 文件（参考 `backend/.env` 和 `frontend/.env`）：

**Backend (`backend/.env`)**:

```env
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_anon_key
DASHSCOPE_API_KEY=your_dashscope_api_key
SMTP_SERVER=smtp.qq.com
SMTP_PORT=465
SENDER_EMAIL=your_email@qq.com
SENDER_PASSWORD=your_email_password
```

**Frontend (`frontend/.env`)**:

```env
VITE_API_BASE_URL=http://localhost:8000
VITE_PORT=5173
```

### 4. 启动服务

在项目根目录下运行：

```bash
docker-compose up --build
```

启动成功后：

- **前端访问**: [http://localhost:5173](http://localhost:5173)
- **后端 API 文档**: [http://localhost:8000/docs](http://localhost:8000/docs)

## 手动开发运行

### 后端

```bash
cd backend
pip install -r requirements.txt
python -m uvicorn app.main:app --reload --port 8000
```

### 前端

```bash
cd frontend
npm install
npm run dev
```

## 目录结构

- `backend/`: FastAPI 服务、爬虫、LLM 逻辑
  - `api/`: API 路由
  - `models/`: Pydantic 数据模型
  - `services/`: 核心业务逻辑 (Paper, User, Report)
  - `spiders/`: Scrapy 爬虫
  - `prompt/`: LLM 提示词模板
- `frontend/`: React 应用
  - `src/components/`: UI 组件
  - `src/services/`: API 客户端
  - `src/types/`: TypeScript 类型定义

## 许可证

Alpache-2.0 License

## TODO

### 测试用户

tsingxiii@gmail.com
Vip112233

导入功能：
允许用户从 Google Scholar、ORCID 导入关注作者
从已有论文库导入关键词

用户反馈模型，需要存储用户的交互与反馈过程

### 2025-11-29 22:17:32

TODO

- 测试好论文分析 prompt，完善公共数据库的论文
- 完善私有数据库构建过程
  1. 检索对应类别的公共论文（先以 cl 类别为例）
  2. 对检索到的论文进行分析，明确需要更新的字段（id、category 等）
  3. 更新私有数据库
- 确保前端正常显示查询到的论文
  1. 卡片显示（支持模糊效果，缩小显示大小），根据相关度排序，支持显示未被选中的论文（也按评分排序）
  2. 支持点击进入详情页，详情页支持左右滑动
  3. 支持 like\dislike 等，并更新到数据库中存储交互行为（更新 dislike 反馈面板）
- 支持通过自然语言更新需求（看调用什么 LLM，也需要提前写 prompt）
- 生成报告功能

  1. 编写 prompt，在 cherry studio 测试一下，直接输入论文所有信息，看能否在长上下文下直接生成报告（好像有直接帮助生成报告的开源软件，deep Research 相关的）
  2. 前端正常显示报告，能够索引到对应论文（这个前后端逻辑肯定需要更改）
  3. 能够直接在前端点击生成研报，如果当日有就不允许生成

- 优化前端显示效果

  1. 缩小的一定程度会丢失顶栏，elicit 到一定程度自动转为移动端样式
  2. 优化交互反馈
  3. 昵称显示栏，自然语言显示栏优化

- 可能后面还需要更换前端语言，前端的反应速度太慢了，看看 node.js 有没有用，好不好实现（上线后的优化）

### 2025-12-5 14:52:35

是否要将每日更新的论文都放在独立的数据库中？然后如果当日有更新就将过去的论文丢到旧的数据库中？（暂时没必要，只能算优化）
或者爬虫将今日获取的论文 id 与对应类别存入缓存，然后筛选机制直接在这其中比对类别？然后将符合要求的论文 id 获取公共数据库中的详情信息，再分析后更新到私有数据库中？
再或者不要缓存，直接额外维护一个公共数据库，存放每日更新论文。每次获取新论文后，同时存入每日更新数据库和公共数据库，只是每日更新数据库是覆盖操作（先清空再写入），而公共数据库是存入操作

目前写的 类别筛选论文 功能还需要筛掉没有初始化的论文，后续需要每日爬取的论文直接分析过后再更新到数据库中，就不用再考虑 status 字段了

我需要额外维护一个公共数据库，存放每日更新论文。每次获取新论文后，同时存入每日更新数据库和公共数据库，只是每日更新数据库是覆盖操作（先清空再写入），而公共数据库是存入操作。这样一来，为每个用户检索当日更新论文时，就可以直接从每日更新数据库中检索，在总论文库数量很多时，能减少很多计算量。详细来说，后续需要先写测试，测试 arxiv 是否有所更新（比对 new 网站的 html，看是否有更新），只有更新了，才调用爬虫程序爬取更新后的论文。具体来说，测试是否更新（若更新） → 清空每日论文数据库 → 爬取新论文 → 存入每日论文数据库 → 获取详情信息 → 调用 LLM 分析 → 存入公共数据库。

你需要：

1. 额外创建一个数据库，用于存放每日更新论文
2. 实现测试是否更新功能：初始化一个 str，每次获取论文时要储存 “Showing new listings for” 后的时间。在调用时在https://arxiv.org/list/cs.CL/new 页面中，比对 html 元素中 “Showing new listings for” 后的时间，看是否更新（比如今天的是 Showing new listings for Friday, 5 December 2025）。如果确认 arxiv 有更新，就需要调用爬虫和后续的更新操作
3. 爬虫爬取论文后，写入每日更新数据库，将后续关于公共数据库的更新操作都关联到每日更新数据库
4. 在使用 LLM 提取完所有论文信息后，存入到公共数据库中

我需要你确认现在的代码逻辑，是否满足以上需求
收到后，先向我确认是否有其他需要我确认的问题与可以优化项目的选项，并给出你的推荐。我给出反馈后你再生成中文版 plan
你认为功能还可以怎样进行丰富？
这个指令是否有问题/冲突

### 指令

- 我需要代码的逻辑是：根据类别获取论文时，是从 papers 共有论文库中找寻 status 为 completed 或 analyzed 字段且 category 标签和用户关注的 category 有交集的论文，把这些 id 打包输出。并且还要比对 user_paper_states 对应 user_id 下存在的论文，确保不输出重复的论文。你需要确认是否满足这个功能。我猜测可能在获取论文后，程序又进行了去重操作。实际上应该是把找到的论文去重了（我目前设置的默认获取论文数为 1，方便测试，你不要改动）

### 2025-12-05 22:41:28

实现前端显示 → 报告生成 → 邮件发送 → 前端显示

### 2025-12-06 21:09:28

更换 api
生成多类别论文
生成报告 -- 将前固定数量的论文输入给模型，每句话都需要有引用，需要固定一个报告板式
实现邮件发送功能 -- html 格式

### 2025-12-07 20:53:19

报告生成流程

1. 借助 AI 生成符合预期的内容
2. 实现前端的解析
3. 后端实现能够稳定生成对应内容的 prompt

### 2025-12-08 09:49:22

为每个用户初始化通用的报告？还是初始化页面？就是在获取不到用户数据的时候显示初始化报告
简化用户注册流程

### 2025-12-09 09:09:24
